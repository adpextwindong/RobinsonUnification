%http://ropas.snu.ac.kr/lib/dock/Ro1965.pdf
\documentclass[8pt]{extarticle}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{titlesec}
\usepackage{enumitem}
\newlist{steps}{enumerate}{1}
\setlist[steps, 1]{label = Step \arabic*.}

%https://tex.stackexchange.com/a/164334
%TODO style subsection's more
\titleformat{\subsection}[runin]
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}[runin]
  {\normalfont\large\bfseries}{\thesubsubsection}{1em}{}

\geometry{
    paperheight=9.75in,
    paperwidth=6.25in,
    left=0.90in,
    top=0.5in
}

%https://tex.stackexchange.com/a/53981
\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{p5herbrandt}{}
\newcustomtheorem{p5theorem}{Theorem}       %Page 5 Theorem 1
\newcustomtheorem{grt}{}                    %Page 6 Ground Resolution Theorem
\newcustomtheorem{p7theorem}{Theorem}       %Page 7 Theorem 2
\newcustomtheorem{p7lemma}{}                %Page 7 Lemma
\newcustomtheorem{p7corollary}{Corollary}   %Page 7 Corollary
\newcustomtheorem{pg8theorem3}{Theorem}     %Page 8 Theorem 3
\newcustomtheorem{pg8rt}{Resolution Theorem}%Page 8 Resolution Theorem
\newcustomtheorem{unificationTheorem}{}     %Page 11 Unification Theorem
\newcustomtheorem{p15theorem}{}             %Page 15 Purity Theorem

%TODO fix horizontal spacing of title
\title{\textbf{ A Machine-Oriented Logic Based on the Resolution Principle}}
\author{J. A. Robinson}
\date{\emph{Argonne National Laboratory* and Rice University$\dagger$}}

\begin{document}

\maketitle


\textit{Abstract}. Theorem-proving on the computer, using procedures based on the fundamental theorem of Herbrand concerning the first-order predicate calculus, is examined with a view towards improving the efficiency and widening the range of practical applicability of these procedures. A close analysis of the process of substitution (of terms for variables), and the process of truth-functional analysis of the results of such substitutions, reveals that both processes can be combined into a single new process (called \emph{resolution}), iterating which is vastly more efficient than the older cyclic procedures consisting of substitution stages alternating with truth-functional analysis stages.

The theory of the resolution process is presented in the form of a system of first-order logic with just one inference principle (the resolution principle). The completeness of the system is proved; the simplest proof-procedure based on the system is then the direct implementation of the proof of completeness. However, this procedure is quite inefficient, and the paper concludes with a discussion of several principles (called search principles) which are applicable to the design of efficient proof-procedures employing resolution as the
basic logical process.

\section{Introduction}
Presented in this paper is a formulation of first-order logic which is specifically designed for use as the basic theoretical instrument of a computer theorem-proving program. Earlier theorem-proving programs have been based on systems of first-order logic which were originally devised for other purposes. A prominent feature of those systems of logic, which is lacking in the system described in this paper, is the relative \emph{simplicity} of their inference principles.

Traditionally, a single step in a deduction has been required, for pragmatic and psychological reasons, to be simple enough, broadly speaking, to be apprehended as correct by a human being in a single intellectual act. No doubt this custom originates in the desire that each single step of a deduction should be indubitable, even though the deduction as a whole may consist of a long chain of such steps. The ultimate conclusion of a deduction, if the deduction is correct, follows logically from the premisses used in the  deduction; but the human mind may well find the unmediated transition from the premisses to the conclusion surprising, hence (psychologically) dubitable. Part of the point, then, of the logical analysis of deductive reasoning has been to reduce complex inferences, which are beyond the capacity of the human mind to grasp as single steps, to chains of simpler inferences, each of which is within the capacity of the human mind to grasp as a
single transaction.
\vspace{0.3cm}

Work performed under the auspices of the U. S. Atomic Energy Commission.

* Argonne, Illinois.

$\dagger$ Present address: Rice University, Houston, Texas.

\vspace{0.2in}
%TODO fix firstpage footer
Journal of the Association for Computing Machinery, Val. 1 2, No. 1 (January, 1965), pp. 23-41

\newpage

From the theoretical point of view, however, an inference principle need only be \emph{sound} (i.e., allow only logical consequences of premisses to be deduced from them) and \emph{effective} (i.e., it must be algorithmically decidable whether an alleged application of the interference principle is indeed an application of it). When the agent carrying out the application of an inference principle is a modern computing machine, the traditional limitation on the complexity of inference principles is no longer very appropriate. More powerful principles, involving perhaps a much greater amount of combinatorial information-processing for a single application, become a possibility.

In the system described in this paper, one such inference principle is used. It is called the \emph{resolution principle}, and it is machine-oriented, rather than human-oriented, in the sense of the preceding remarks. The resolution principle is quite powerful, both in the psychological sense that it condones single inferences which are often beyond the ability of the human to grasp (other than discursively), and in the theoretical sense that it alone, as sole inference principle, forms a complete system of first-order logic. While this latter property is of no great importance, it is interesting that (as far as the author is aware) no other complete system of first-order logic has consisted of just one inference principle, if one construes the device of introducing a logical axiom, given outright, or by a schema, as a (premiss-free) inference principle.

The main advantage of the resolution principle lies in its ability to allow us to avoid one of the major combinatorial obstacles to efficiency which have plagued earlier theorem-proving procedures.

In Section 2 the syntax and semantics of the particular formalism which is used in this paper are explained.

\section{Formal Preliminaries}

The formalism used in this paper is based upon the notions of unsatisfiability and refutation rather than upon the notions of validity and proof. It is well known (cf. \cite{davis_1960} and \cite{robinson_1963}) that in order to determine whether a finite set $S$ of sentences of first-order logic is satisfiable, it is sufficient to assume that each sentence in $S$ is in prenex form with no existential quantifiers in the prefix; moreover the matrix of each sentence in $S$ can be assumed to be a disjunction of formulas each of which is either an atomic formula or the negation of an atomic formula. Therefore our syntax is set up so that the natural syntactical unit is a finite set $S$ of sentences in this special form. The quantifier prefix is omitted from each sentence, since it consists just of universal quantifiers binding each variable in the sentence; furthermore the matrix of each sentence is regarded simple as the set of its disjuncts, on the grounds that the order and multiplicity of the disjuncts in a disjunction are immaterial.

Accordingly we introduce the following definitions (following in part the nomenclature of \cite{davis_1960} and \cite{robinson_1963}):\\
\subsection{Variables}. The following symbols, in alphabetical order, are variables:
\begin{align*}
    u, v, w, x, y, z, u_1 , v_1 , w_1 , x_1 , y_1 , z_1 , u_2 , \dotsm , etc.
\end{align*}
\newpage

\subsection{Function symbols}. The following symbols, in alphabetical order, are function symbols of degree $n$, for each $n\geqq0$:
\begin{align*}
    a^n,b^n,c^n,d^n,e^n,f^n,g^n,h^n,k^n,a_1^n,b_1^n,\dotsm, etc.
\end{align*}

\noindent When $n = 0$, the superscript may be omitted. Function symbols of degree 0 are \emph{individual constants}.

\subsection{Predicate symbols}. The following symbols, in alphabetical order, are predicate symbols of degree $n$, for each $n\geqq0$:
\begin{align*}
    P^n,Q^n,R^n,P_1^n,Q_1^n,R_1^n,P_2^n,\dotsm, etc.
\end{align*}

\noindent The superscript may be omitted when $n$ is 0.

\subsection{The negation symbol}. The following symbol is the negation symbol: $\sim$

\subsection{Alphabetical order of symbols}. The set of all symbols is well ordered in alphabetical order by adding to the above ordering conventions the rule that variables precede function symbols, function symbols precede predicate symbols, predicate symbols precede the negation symbol, function symbols of lower degree precede function symbols of higher degree, and predicate symbols of lower degree precede predicate symbols of higher degree.

\subsection{Terms}. A variable is a term, and a string of symbols consisting of a function symbol of degree $n\geqq0$ followed by $n$ terms is a term.

\subsection{Atomic formulas}. A string of symbols consisting of a predicate symbol of degree $n\geqq0$ followed by $n$ terms is an atomic formula.

\subsection{Literals}. An atomic formula is a literal; and if $A$ is an atomic formula then $\sim A$ is a literal.

\subsection{Complements}. If $A$ is an atomic formula, then the two literals $A$ and $\sim A$ are said to be each other's complements, and to form, in either order, a complementary pair.

\subsection{Clauses}. A finite set (possible empty) of literals is called a clause. The empty clause is denoted by: $\square$

\subsection{Ground literals}. A literal which contains no variables is called a ground literal.

\subsection{Ground clauses}. A clause, each member of which is a ground literal, is called a ground clause. In particular $\square$ is a ground clause.

\subsection{Well-formed expressions}. Terms and literals are (the only) well formed expressions.

\subsection{Lexical order of well-formed expressions}. The set of all well formed expressions is well ordered in lexical order by the rule that $A$ precedes $B$ just in case that $A$ is shorter than $B$ or, if $A$ and $B$ are of equal length, then $A$ has the alphabetically earlier symbol in the first symbol position at which $A$ and $B$ have distinct symbols.

In writing well-formed expressions for illustrative purposes, we follow the more readable plan of enclosing the $n$ terms following a function symbol or predicate symbol of degree $n$ by a pair of parentheses, separating the terms, if there are two or more, by commas. We can then unambiguously omit all superscripts from symbols. In writing finite sets, we follow the usual convention of

\newpage
\noindent enclosing the members in a pair of braces and of separating the members by commas, with the understanding that the order of writing the members in immaterial.

\subsection{Herbrand universes}. With any set $S$ of clauses there is associated a set of ground terms called the Herbrand universe of $S$, as follows: let $F$ be the set of all function symbols which occur in $S$. If $F$ contains any function symbols of degree 0, the functional vocabulary of $S$ is $F$; otherwise it is the set ${a}\cup F$. The Herbrand universe of $S$ is then the set of all ground terms in which there occur only symbols in the functional vocabulary of $S$.

\subsection{Saturation}. If $S$ is any set of clauses and $P$ is any set of terms, then by $P(S)$ we denote the saturation of $S$ over $P$, which is the set of all ground clauses obtainable from members of $S$ by replacing variables with members of P-occurrences of the same variable in any one clause being replaced by occurrences of the same term.

\subsection{Models}. A set of ground literals which does not include a complementary pair is called a model. If $M$ is a model and $S$ is a set of ground clauses, then $M$ is a model of $S$ if, for all $C$ in $S$, $C$ contains a member of $M$. Then, in general, if $S$ is any set of clauses, and $H$ is the Herbrand universe of $S$, we say that $M$ is a model of $S$ just in case that $M$ is a model of $H(S)$.

\subsection{Satisfiability}. A set $S$ of clauses is satisfiable if there is a model of $S$; otherwise $S$ is unsatisfiable.

From the definition of satisfiability, it is clear that any set of clauses which contain $\square$ is unsatisfiable, and that the empty set of clauses is satisfiable. These two circumstances will appear quite natural as the development of our system proceeds. It is also clear that according to our semantic definitions each nonempty clause is interpreted, as explained in the informal remarks at the beginning of this section, as the universal closure of the disjunction of the literal which it contains.

\subsection{Ground resolvents}. If $C$ and $D$ are two ground clauses, and $L\subseteq C,M\subseteq D$ are two singletons (unit sets) whose respective members form a complementary pair, then the ground clause: $(C - L)\cup (D - M)$ is called a ground resolvent of $C$ and $D$.

Evidently any model of ${C,D}$ is also a model of ${C,D,R}$, where $R$ is a ground resolvent of $C$ and $D$. Not all pairs of ground clauses have ground resolvents, and some have more than one; but in no case, as is clear from the definitions, can two ground clauses have more than a finite number of ground resolvents.

\subsection{Ground resolution}. If $S$ is any set of ground clauses, then the ground resolution of $S$, denoted by $\mathscr{R}(S)$, is the set of ground clauses consisting of the members of $S$ together with all ground resolvents of all pairs of members of $S$.

\subsection{$N$-th ground resolution}. If $S$ is any set of ground clauses, then the $n$th ground resolution of $S$, denoted by $\mathscr{R}^n(S)$, is defined for each $n\geqq 0$ as follows: $\mathscr{R}^0(S) = S$; and for $n\geqq 0$, $\mathscr{R}^{n+1} = \mathscr{R}(\mathscr{R}^n(S))$.

This completes the first batch of definitions. The next sections are concerned with the various forms that Herbrand's Theorem takes on in our system. To each such form, there is a type of refutation procedure which that form suggests

\newpage

\noindent and justifies. The basic version is stated as follows (cf. \cite{davis_1960,gilmore_1960}):

\begin{p5herbrandt}{Herbrand's Theorem}\label{p5herbrandt}
If $S$ is any finite set of clauses and $H$ its Herbrand universe, then $S$ is unsatisfiable if and only if some finite subset of $H(S)$ is unsatisfiable.
\end{p5herbrandt}

\section{\emph{Saturation Procedures}}

Is was noted in an earlier paper \cite{robinson_1963} that one can express Herbrand's Theorem in the following form:

\begin{p5theorem}{1}\label{p5theorem}
If S is any finite set of clauses, then S Ls unsatisfiable if and only if, for some finite subset P of the Herbrand universe of S, P(S) is unsatisfiable. 
\end{p5theorem}

This version of Herbrand's Theorem suggests the following sort of refutation procedure, which we call a \emph{saturation procedure}: given a finite set $S$ of clauses, select a sequence $P_0, P_1, P_2, \dotsm ,$ of finite subsets of the Herbrand universe $H$ of $S$, such that $P_j \subseteq P_{j+1}$ for each $j\geqq$, and such that  $\cup_{j=0}^\infty P_j = H$. Then examine in turn the sets $P_0(S),P_1(S),P_2(S), \dotsm ,$ for satisfiability. Evidently, for any finite subset $P$ of $H$, $P \subseteq P_j$ for some $j$, and therefore $P(S) \subseteq P_j(S)$. Therefore, by Theorem 1, if $S$ is unsatisfiable then, for some $j$, $P_j(S)$ is unsatisfiable.

Of course, any specific procedure of this sort must make the selection of \\$P_0, P_1, P_2,\dotsm$, uniformly for all finite sets of clauses. A  particularly natural way of doing this is to use the so-called levels $H_0, H_1, H2,\dotsm$, of the Herbrand universe $H$; where $H_0$ consists of all the individual constants in $H$, and $H_{n+1}$, for $n\geqq0$, consists of all the terms in $H$ which are in $H_n$, or whose arguments are in $H_n$. In \cite{robinson_1963} we called procedures using this method \emph{level-saturation procedures}. It, was there remarked that essentially the procedures of Gilmore \cite{gilmore_1960} and Davis Putnam \cite{davis_1960} are level-saturation procedures. 

The major combinatorial obstacle to efficiency for level-saturation procedures is the enormous rate of growth of the finite sets $H_j$ and $H_j(S)$ as $j$ increases, at least for most interesting sets $S$. These growth rates were analyzed in some detail in \cite{robinson_1963}, and some examples were there given of some quite simple unsatisfiable S for which the earliest unsatisfiable  $H_j(S)$ is so large as to be absolutely beyond the limits of feasibility.

An interesting heuristic remark is that, for every finite set $S$ of clauses which is unsatisfiable and which has a refutation one could possibly construct, there is at least one reasonably small finite subset of the Herbrand universe of $S$ such that $P(S)$ is unsatisfiable and such that $P$ is \emph{minimal} in the sense that $Q(S)$ is satisfiable for each proper subset $Q$ of $P$. Such a $P$ was called a \emph{proof set} for $S$ in \cite{robinson_1963}. If only, then, a benevolent and omniscient demon were available who could provide us, in reasonable time, with a proof set $P$ for each unsatisfiable finite set $S$ of clauses that we considered, we could simply arrange to saturate $S$ over $P$ and then extract a suitable refutation of $S$ from the resulting finite unsatisfiable set $P(S)$ of ground clauses. This was in fact the underlying scheme of a computer program reported in \cite{robinson_1963}, in which the part of the demon is played, as best his ingenuity allows, by the mathematician using the program. What is really

\newpage

wanted, to be sure, is a simulation of the proof set demon on the computer; but this would appear, intuitively, to be out of the question. 

It turns out that it, is not completely out of the question. In fact, the method developed in the remainder of this paper seems to come quite close to supplying the required demon as a computing process. In Section 4 we take the first major step towards the development of this method by proving more versions of Herbrand's Theorem. We also give a preliminary informal account of the rest of the argument, pending a rigorous treatment in succeeding sections.

\section{The Resolution Theorems and the Basic Lemma}

As a specific method for testing a finite set of ground clauses for satisfiability,
the method of Davis-Putnam \cite{davis_1960} would be hard to improve on from the point
of view of efficiency. However, we now give another method, far less efficient
than theirs, which plays only a theoretical role in our development, and which
is much simpler to state: given the finite set $S$ of ground clauses, form successively
the sets $S, \mathscr{R}(S), \mathscr{R}^2(S),\dotsm $, until either some $\mathscr{R}^n(S)$ contains $\square$, or does not
contain $\square$ but is equal to $\mathscr{R}^{n+1}(S)$. In the former case, $S$ is unsatisfiable; in the
latter case, $S$ is satisfiable. One or other of these two terminating conditions
must eventually occur, since the number of distinct clauses formable from the
finite set of literals which occur in $S$ is finite, and hence in the nested infinite
sequence:

\subsection{} $S \subseteq \mathscr{R}(S) \subseteq \mathscr{R}^2(S) \subseteq \dotsm \subseteq \mathscr{R}^n(S) \subseteq \dotsm$,\\not all of the inclusions are proper, since resolution introduces no new literals.

In view of the finite termination of the described process we can prove its
correctness, as stated above, in the form of the ground resolution theorem. 


\begin{grt}{Ground Resolution Theorem}\label{grt}
If $S$ is any finite set of ground clauses, then $S$ is unsatisfiable if and only if $\mathscr{R}^n(S)$ contains $\square$, for some $n\geqq O$. 
\end{grt}

PROOF. The "if" part is immediate. To prove the "only if" part, let $T$ be the
terminating set $\mathscr{R}^n(S)$ in the sequence (4.1) above, so that $T$ is closed under ground resolution. We need only show that if $T$ does not contain $\square$, then $T$ is satisfiable, and hence $S$ is satisfiable since $S$ $\subseteq$ $T$. Let $L_1, \dotsm , L_k$ be all the distinct atomic formulas which occur in $T$ or whose complements occur in $T$. Let $M$ be the model defined as follows: $M_0$ is the empty set; and for $0 < j \leq k$, $M_j$ is the set $M_{j-1} \cup {L_j}$, unless some clause in $T$ consists entirely of complements of literals in the set $M{j_1}\cup{\sim L_j}$. in which case $M_j$ is the set $M_{j-1}\cup{\sim L_j}$. Finally, $M$ is $M_k$. Now if $T$ does not contain $\square$, $M$ satisfies $T$. For otherwise there is a least $j$, $0 < j \leq k$, such that some clause (say, $C$) in $T$ consists entirely of complements of literals in the set $M_j$. By the definition of $M_j$, therefore, $M_j$ is $M_{j-1} \cup {\sim L_j}$. Hence by the leastness of $j$, $C$ contains $L_j$. But since $M_j$ is $M_{j-1}\cup {\sim L_j}$, there is some clause (say, $D$) in $T$ which consists entirely of complements of literals in the set $M{j_1}\cup {L_j}$. Hence by the leastness of $j$, $D$ contains $\sim L_j$. Then the clause $B = (C - {L_j}) \cup (D - {\sim L_j})$ consists entirely of complements of literals in the set $M_{j-1}$ , unless $B$ is $\square$. But $B$ is a

\newpage

ground resolvent of $C$ and $D$, hence is in $T$, hence is not $\square$. Thus the leastness of $j$ is contradicted and the theorem is proved.

The Ground Resolution Theorem now allows us to state a more specific form
of Theorem 1, namely,


\begin{p7theorem}{2}\label{two}
If $S$ is any finite set of clauses, then $S$ is unsatisfiable if and only
if, jut some finite subset $P$ of the Herbrand universe of $S$, and some $n \geqq O$, $\mathscr{R}^n(P(S))$ contains $\square$.
\end{p7theorem}

It is now possible to state informally the essential steps of the remaining part of the development. We are going to generalize the notions of ground resolvent and ground resolution, respectively, to the notions of resolvent and resolution. by removing the restriction that the clauses involved be only ground clauses. Any two clauses will then have zero, one or more clauses as their resolvents, but in no case more than finitely many. In the special ease that $C$ and $D$ are ground clauses, their resolvents, if any, are precisely their ground resolvents as already defined. Similarly, the notations $\mathscr{R}(S), \mathscr{R}(S)$ will be retained, with $S$ allowed to be any set of clauses. $\mathscr{R}(S)$ will then denote the resolution of $S$, which
is the set of clauses consisting of all members of $S$ together with all resolvents of all pairs of members of $S$. Again, $\mathscr{R}(S)$ is precisely the ground resolution of $S$, already defined, whenever $S$ happens to be a set of ground clauses. 

The details of how this generalization is done must await the formal definitions in Section 5. However, an informal grasp of the general notion of resolution is obtainable now, prior to its exact treatment, from simply contemplating the fundamental property which it will be shown to possess: \emph{resolution is semicommutative with saturation.} More exactly, this property is as stated in the following basic Lemma, which is proved in Section 5:

\begin{p7lemma}{Lemma}\label{p7lemma}
If $S$ is any set of clauses, and $P$ is any subset of the Herbrand universe of $S$, then: $\mathscr{R}(P(S)) \subseteq P(\mathscr{R}(S))$.
\end{p7lemma}

The fact is, as will be shown here, that any ground clause which can be obtained by \emph{first} instantiating over P a pair $C$, $D$ of clauses in $S$, and \emph{then} forming a ground resolvent of the two resulting instances, cast also be obtained by instantiating over $P$ one of the finitely many resolvents of $C$ and $D$.

It is an easy corollary of the basic Lemma that nth resolutions are also semicommutative with saturation: 

\begin{p7corollary}{}\label{p7corollary}
If $S$ is any set of clauses and $P$ is any subset of the Herbrand
universe of $S$, then: $\mathscr{R}^n(P(S)) \subseteq P(\mathscr{R}^n(S))$ for all $n \geqq O$.
\end{p7corollary}

%https://tex.stackexchange.com/a/27284
\begin{proof}
    By induction on $n$. $\mathscr{R}^0(P(S)) = P(S) = P(\mathscr{R}^0(S))$, so that the case $n = 0$ is trivial. And if, for $n \geqq 0$, $\mathscr{R^n}(P(S)) \subseteq P(\mathscr{R}^n(S))$, then:
    \begin{align*}
        \mathscr{R}^{n+1}(P(S)) &= \mathscr{R}(\mathscr{R}^n(P(S))) && \text{by definition of $\mathscr{R}^{n+1}$}\\
                                &\subseteq \mathscr{R}(P(\mathscr{R}^n(S))) && \text{by the induction hypothesis, as $\mathscr{R}$ preserves inclusion,}\\
                                &\subseteq P(\mathscr{R}(\mathscr{R}^n(S))) && \text{by the Lemma,}\\
                                &= P(\mathscr{R}^{n+1}(S)) && \text{by definition of $\mathscr{R}^{n+1}$,}
    \end{align*}
\end{proof}
\noindent and the Corollary is proved.
\newpage

Now by the above Corollary to the basic Lemma we may immediately obtain
a third version, of Herbrand's Theorem from Theorem 2: 

\begin{pg8theorem3}{3}\label{pg8theorem3}
If $S$ is any finite set of clauses, then $S$ is unsatisfiable if and only if, for soem finite subset $P$ of the Herbrand universe of $S$, and some $n\geqq 0$, $P\mathscr{R}^n(S)$ contains $\square$.
\end{pg8theorem3}

Here, the order of the saturation and $n$th resolution operations is reversed.
Now a rather surprising simplification of Theorem 3 can be made, on the basis
of the remark that mere replacement of variables by terms cannot produce $\square$
from a nonempty clause. Hence $P(\mathscr{R}^n(S))$ will contain $\square$ if and only if $\mathscr{R}^n(S)$
contains $\square$. From Theorem 3, therefore, we immediately obtain our final version
of Herbrand's Theorem, which is the main result of this paper, and which we
call:


\begin{pg8rt}
If $S$ is any finite set of clauses, then S is unsatisfiable
if and only if $\mathscr{R}^n(S)$ contains $\square$, for some $n\geqq 0$. 
\end{pg8rt}

The statement of the Resolution Theorem is just that of the Ground Resolution Theorem with the word "ground" omitted. Apart, therefore, from the somewhat more complex way in which the resolvents of two clauses are computed
(described in Section 5) the method suggested by the Resolution Theorem for
testing a finite set S of clauses for unsatisfiability is exactly like that given earlier for the ease that S is a set of ground clauses, and indeed it. automatically reverts to that method when it is applied to a finite set of ground clauses. However, it is no longer the ease in general that the nested sequence

\begin{align*}
    S \subseteq \mathscr{R}(S) \subseteq \mathscr{R}^2(S) \subseteq \dotsm \subseteq \mathscr{R}^n(S) \subseteq \dotsm
\end{align*}

must terminate for all finite $S$. By Church's Theorem this could not be so, for otherwise we would have a decision procedure for satisfiability for our formulation of first-order logic. 

Consider now the "proof set demon" discussed in Section 3. We there supposed that if see were given a proof set $P$ for an unsatisfiable set $S$ of clauses, all we would have to do would be to compute until we encountered the first $\mathscr{R}^n(P(S))$ which contains $\square$, in order to obtain from it a formal refutation of $S$. But the Resolution Theorem assures us that by the time we had computed ,$\mathscr{R}^n(S)$, if not before, we would have turned up $\square$, despite our ignorance of $P$. In this sense the Resolution Theorem makes the proof set demon's role unnecessary.

In Section 5 we introduce a little more formal apparatus by a second batch of definitions, and pay off our debts by defining the general notion of resolution and proving the basic Lemma.

\section{Substitution, Unification and Resolution}

The following definitions are concerned with the operation of instantiation, i.e. substitution of terms for variables in well-formed expressions and in sets of well-formed expressions, and with the various auxiliary notions needed to define resolution in general.

\subsection{Substitution components.} A substitution component is any expression of
\newpage

\noindent the form $T/V$, where $V$ is any variable and $T$ is any term different from $V$. $V$ is
called the \emph{variable} of the component $T/V$, and $T$ is called the \emph{term} of the component $T/V$.

\subsection{Substitutions.}

A substitution is any finite set (possibly empty) of substitution components none of the variables of which are the same. If $P$ is any set;
of terms, and the terms of the components of the substitution $\theta$ are all in $P$, we say" that $\theta$ is a substitution over $P$. We write the substitution whose components are $T_1/V_1, \dotsm , T_k/V_k as \{T_1/V_1, \dotsm, T_k/V_k\}$, with the understanding that the order of the components is immaterial. We use lowercase Greek letters to denote substitutions. In particular, $\epsilon$ is the \emph{empty substitution}.

\subsection{Instantiation.}

IF E is any finite string of symbols and

\begin{align*}
    \theta = \{T_1/V_1, \dotsm, T_k/V_k\}
\end{align*}

%TODO double check the subscript
is any substitution, then the instantiation of $E$ by $\theta$ is the operation of replacing each occurrence of the variable $V_i$, $1 \leq i \leq k$, in $E$ by an occurrence of the term $T_i$. The resulting string, denoted by $E\theta$, is called the instance of $E$ by $\theta$. I.e., if $E$ is the string $E_0V_{i_1}E_1 \dotsm V_{i_n}E_n$ then $E\theta$ is the string $E_\theta T_{i_1}E_1\dotsm T_{i_n}E_n$. Here, none of the substrings $E_i$ of $E$ contain occurrences of the variables $V_1, \dotsm , V_k$ some of the $E_i$ are possibly null, $n$ is possibly $0$, and each $V_{i_i}$ is an occurrence of one of the variables $V_i,\dotsm V_k$. Any string $E\theta$ is called an instance of the
string $E$. If $C$ is any set of strings and $\theta$ a substitution, then the instance of $C$ by $\theta$ is the set of all strings $E0$, where $E$ is in $C$. We denote this set. by $C\theta$, and say that it is an instance of $C$.

\subsection{Standardization.}

If $C$ is any finite set of strings, and $V_1, \dotsm , V_k$ are
all the distinct variables, in alphabetical order, which occur in strings in $C$, then
the $x$-standardization of $C$, denoted by $\xi_c$, is the substitution
$x_1/V_1,\dotsm,x_k/V_k$
and the $y$-standardization of $C$, denoted by $\eta_c$, is the substitution
\begin{align*}
    \{y_1/V_1,\dotsm,y_k/V_k\}
\end{align*}

\subsection{Compositions of substitutions.}

If $\theta = \{T_1/V_1,\dotsm T_k/V_k\}$ and $\lambda$ are any
two substitutions, then the set $\theta^\prime \cup \lambda^\prime$ ,where $\lambda^\prime$ is the set of all components of $\lambda$
whose variables are not among $V_1,\dotsm V_k$, and $\theta^\prime$ is the set of all components
$T_i\lambda/V_i, 1 \leq i \leq k$, such taht $T_i\lambda$ is different from $V_i$, is called the composition of $\theta$ and $\lambda$, and is denoted by $\theta\lambda$

It is straight forward to verify that $\epsilon\theta = \theta\epsilon = \theta$ for any substitution $\theta$. Also,
composition of substitutions enjoys the associative property $(\theta\lambda)\mu = \theta(\lambda\mu)$, so
that we may omit parentheses in writing multiple compositions of substitutions. 
These properties of the composition of substitutions are established by the following propositions.

\subsubsection{}
$(E\sigma)\lambda = E(\sigma\lambda)$ \emph{for all strings E and all} substitutions $\sigma, \lambda$

\begin{proof}
Let $\sigma = \{T_1/V_1,\dotsm,T_k/V_k\}$, $\lambda = \{U_1/W_1,\dotsm,U_m/W_m\}$ and\\$E = E_oV_i,E_1,\dotsm V_{i_n}E_n$ as explained in (5.3) above. Then by definition $E\sigma =$
\end{proof}

%TODO use this as a ref for the hard to read subscript
\newpage
$E_0T_{1_1}E_1\dotsm T_{i_n}E_n$, and $(E\sigma)\lambda = \overline{E}_0\overline{T}_{i_1}\overline{E}_1\dotsm\overline{T}_{i_n}\overline{E}_n$, where each $\overline{T}_{i_i}$ is $T_{i_i}\lambda_i$ and each $\overline{E}_i$ is $E_i\lambda^\prime$, where $\lambda^\prime$ is the set of all components of $\lambda$ whose variables are not among $V_1,\dotsm,V_k$ (since none of these variables occur in any $E_i$). But $\sigma\lambda = \sigma^\prime \cup \lambda^\prime$, where each component of $\sigma\prime$ is just $\overline{T}_i/V_i$ whenever $\overline{T}_i$ is different from $V_i$. Hence $E(\sigma\lambda) = \overline{E}_0\overline{T}_{i_i}\overline{E}_1\dotsm\overline{T}_{i_n}\overline{E}_n$

\subsubsection{}
\emph{For any substitutions $\sigma$, $\lambda$: if $E\sigma = E\lambda$ for all strings $E$, then $\sigma = \lambda$}
\begin{proof}
Let $V_1,\dotsm,V_k$ include all the variables of the components of $\sigma$ and $\lambda$; then $V_j\sigma = V_j\lambda$, for $1 \leq j \leq k$. Then all the components of $\sigma$ and $\lambda$ are the same.
\end{proof}

\subsubsection{}
\emph{For any substitutions $\sigma, lambda, mu: (\sigma\lambda)\mu = \sigma(\lambda\mu)$}
\begin{proof}
Let $E$ be any string. Then by 5.5.1,
\begin{align*}
    E((\sigma\lambda)\mu) &= (E(\sigma\lambda))\\
                          &= ((E\sigma)\lambda)\mu\\
                          &= (E\sigma)(\lambda\mu)\\
                          &= E(\sigma(\lambda\mu)).
\end{align*}
\end{proof}

\noindent Hence $(\sigma\lambda)\mu = \sigma(\lambda\mu)$ by (5.5.2).

We shall also have occasion to use the following distributive property. 
\subsubsection{}
\emph{For any sets $A$, $B$ of strings and substitution $\lambda: (A \cup B)\lambda = A\lambda\cup B\lambda$}

\subsection{Disagreement sets.}

If $A$ is any set of well-formed expressions, we call the
set $B$ the disagreement set of $A$ whenever $B$ is the set of all well-formed subexpressions of the well-formed expressions in $A$, which begin at the first symbol position at which not all well-formed expressions in $A$ have the same symbol.
\noindent Example:

\begin{align*}
                           A &= \{P(x,h(x,y),y),P(x,k(y),y), P(x,a,b)\}\\
\text{Disagreement set of} A &= \{h(x,y), k(y), a\}.
\end{align*}

Evidently, if $A$ is nonempty and is not a singleton, then the disagreement set
of $A$ is nonempty and is not a singleton. 

\subsection{Unification.}
If $A$ is any set of well-formed expressions and $\theta$ is a substitution, then $\theta$ is said to unify $A$, or to be a unifier of $A$, if $A\theta$ is a singleton. Any set of well-formed expressions which has a unifier is said to be unifiable. 

Evidently, if $\theta$ unifies $A$, but $A$ is not a singleton, then $\theta$ unifies the disagreement set of $A$.

\subsection{Unification Algorithm.}
The following process, applicable to any finite
nonempty set $A$ of well-formed expressions, is called the Unification Algorithm:

%https://tex.stackexchange.com/a/341190
\begin{steps}
    \item Set $\sigma_0 = \epsilon$ and $k = 0$, and go to step 2.
    \item If $A\sigma_k$ is not a singleton, go to step 3. Otherwise, set $\sigma_A = \sigma_k$ and terminate.
    \item Let $V_k$ be the earliest, and $U_k$ the next earliest, in the lexical ordering of the disagreement set $B_k$ of $A\sigma_k$ If $V_k$ is a variable, and does not occur in $U_k$, set $\sigma_{k+1} = \sigma_k\{U_k/V_k\}$, add $1$ to $k$, and return to step 2. Otherwise, terminate.
\end{steps}

This definition requires justification in the form of a proof that the given
process is in fact an algorithm. In fact the process always terminates for any

\newpage

finite nonempty set of well-formed expressions, for otherwise there would be
generated an infinite sequence $A, A\sigma_1,A\sigma_2,\dotsm$ of finite nonempty sets of well-formed expressions with the property that each successive set contains one less variable than its predecessor (namely, $A\sigma_k$ contains $V_k$ but $A\sigma_{k+1}$ does not). But this is impossible, since $A$ contains only finitely many distinct variables.

\subsection{Most general unifiers.}
If $A$ is a finite nonempty set of well-formed expressions for which the Unification Algorithm terminates in step 2, the substitution $\sigma A$ then available as output of the Unification Algorithm is called the most
general unifier of $A$, and $A$ is then said to be most generally unifiable.

\subsection{Key triples.}
The ordered triple $\langle L, M, N \rangle$ of finite sets of literals is said
to be a key triple of the ordered pair $\langle C,D\rangle$ of clauses just in case the following conditions are satisfied.

\subsubsection{}
$L$ and $M$ are nonempty, and $L \subseteq C, M \subseteq D$
\subsubsection{}
$N$ is the set of atomic formulas which are members, or complements
of members, of the set $L\xi_c\cup M\eta_d$ (cf. definition (5.4)).
\subsubsection{}
$N$ is most generally unifiable, with most general unifier $\sigma N$.
\subsubsection{}
The sets $L\xi_{c\sigma_N}$ and $M_{\eta_d\sigma_N}$ are singletons whose members are complements.

Evidently, a pair $\langle C, D\rangle$ of clauses has at most a finite number of key triples, and possibly none at all.

\subsection{Resolvents.}
A resolvent of the two clauses $C$ and $D$ is any clause of the
form: $(C - L)\xi_{c\sigma_N} \cup (D - M)_{\eta_D\sigma_N}$ where $\langle L, M, N\rangle$ is a key triple of $\langle C, D\rangle$.
By the remark following definition (5.10) it is dear that two clauses $C$ and $D$ can have at most finitely many resolvents, and possibly none at all.

\subsection{Resolutions.}
If $S$ is any set of clauses then the resolution of $S$, denoted
by $\mathscr{R}(S)$, is the set of all clauses which are members of $S$ or resolvents of members of $S$.

\subsection{$N$-th resolution.}
The $n$\-th resolution of $S$, where $S$ is any set of clauses,
is denoted by $\mathscr{R}^n(S)$ and is defined for all $n \geqq 0$ exactly analogously to definition (2.21).

This completes our second group of definitions. The definition of $\mathscr{R}(S)$ as given is adequate for our theoretical argument, but in practice one would not include in it both the resolvents of $\langle C, D\rangle$ and the resolvents of $\langle D, C\rangle$, since these are in fact identical up to a change of variables. When $C$ and $D$ are both ground clauses, the resolvents of $\langle C, D\rangle$ are actually identical with those of $\langle D, C\rangle$, and are precisely the ground resolvents of $C$ and $D$, as is easily verified.

It now remains to prove the basic Lemma, which will be done after we have
first proved the following theorem establishing the basic property of unification, which we need hi the proof of the Lemma and elsewhere in our theory: 


\begin{unificationTheorem}{Unification Theorem}\label{unificationTheorem}
Let $A$ be any finite nonempty set of well-formed expressions. If $A$ is unifiable, then $\Lambda$ is most generally unifiable with most general
unifier $\sigma_A$; moreover, for any unifier $\theta$ of $A$ there is a substitution $\lambda$ such that
$\theta = \sigma_A\lambda$.
\end{unificationTheorem}

\begin{proof}
It will suffice to prove that under the hypotheses of the theorem the
Unification Algorithm will terminate, when applied to A, at step 2; and that
for each $k \geqq 0$  until the Unification Algorithm so terminates, the equation
\end{proof}

\newpage

THE SCAN OF THIS PAGE IS CUT OFF ON THE RIGHT SIDE

\subsection{}
$\theta = \sigma_k\lambda_k$

\newpage
\noindent
bets, or complements of members, of the set $L\xi_c \cup M_{\eta_D}$. Hence by the Unification Theorem $N$ has a most general unifier $\sigma_N$, and there is a substitution $\lambda$ over $P$ such that $\theta = \sigma_N\lambda$ Hence $L\xi_{C sigma_N}\lambda = L\alpha$ and $M_{\eta D\sigma_N} = M\beta$, and therefore $L\xi_{C\sigma_N}$ and $M_{\eta_D\sigma N}$ are singletons whose members are complements. It follows that $\langle L, M, N\rangle$ is a key triple of $\langle C, D\rangle$, and hence that the clause

\begin{align*}
    B = (C - L)\xi_{C\sigma_N} \cup (D - M)_{\eta D\sigma N}
\end{align*}

is a resolvent of $C$ and $D$; hence $B \in \mathscr{R}(S)$. But since $\theta = \sigma_N\lambda$, it follows by (5.5.4) that $A = B\lambda$ and therefore that $A \in P(\mathscr{R}(S))$. The proof is complete.

The hypotheses of the Lemma do not entail the opposite inclusion $P(\mathscr{R}(S)) \subseteq \mathscr{R}(P(S))$. As a simple counter example, consider:

\begin{align*}
    S = \{\{Q(x,f(y))\},\{\sim Q(g(y),x)\}\}, && P = \{a\}.
\end{align*}

A short investigation shows that $P(\mathscr{R}(S))$ contains $\square$ (since $\mathscr{R}(S)$ does while $\mathscr{R}P(S))$ does not. Thus $S$ is unsatisfiable, but $P$ is not a proof set for $S$.

\section{The Resolution Principle: Refutations}

The single inference principle of our system of logic, mentioned ia Section l,
is the \emph{resolution principle}, namely: \emph{From any two clauses $C$ and $D$, one may infer a resolvent of $C$ and $D$.}

By a \emph{refutation} of the set $S$ of clauses we mean a finite sequence $B_1,\dotsm,B_n$ of clauses such that (a) each $B_i, 1 \leq i \leq n$ is either in $S$ or is a resolvent of two earlier clauses in the sequence, and (b) $B_n$ is $\square$

it is immediate from the Resolution Theorem that a finite set $S$ of clauses is
unsatisfiable if and only if there is a refutation of $S$. Thus the Resolution Theorem is the completeness theorem for our system of logic. 

Two examples of refutations will illustrate the workings of the system. 

%TODO fix indentation
\begin{enumerate}[label=Example \arabic*.]
    \item The set containing just the two clauses $C_1$ and $C_2$, where
        \begin{align*}
            C_1 &= \{Q(x,g(x),y,h(x,y),z,k(x,y,z)\}\\
            C_2 &= \{\sim Q(u,v,e(v),w,f(v,w),x)\}
        \end{align*}
\end{enumerate}
\noindent
has the refutation $C_1, C_2, \square$. Note that $\langle C_1, C_2\rangle$ has the key triple $\langle C_1, C_2, N\rangle$, where $N$ is the set

\begin{align*}
    \{Q(x_1,g(x_1),x_2,h(x_1,x_2),x_3,k(x_1,x_2,x_3)),
    Q(y_1,y_2,e(y_2),y_3,f(y_2,y_3),y_4)\}
\end{align*}
\noindent
The reader can verify in a few minutes of computation with the Unification
Algorithm that $\sigma_N$ is the substitution with the components: 

\begin{align*}
    & y_1/x_1,       &&  h(y_1,e(g(y_1)))/y_2\\
    & g(y_1)/y_2,    &&  f(g(y1), h(y_1, e(g(y_1))))/x_3\\
    & e(g(y_1))/x_2, &&  k(y_1, e(g(y_1)), f(g(y_1), h(y_1, e(g(y_1)))))/y_4
\end{align*}

\noindent
and that then $C_1\xi_{C_1\sigma_N}$ and $C_{2\eta C_2\sigma_N}$ are singletons whose members are complements.

\newpage

This example illustrates the way in which a proof set is automatically computed as a by-product; of the resolution operation. The terms of the above substitution components become those of a proof set for $\{C_1 , C_2\}$ when the variable $y_1$ is replaced throughout by any term of the Herbrand universe of $\{C_1, C_2\}$, e.g. by the individual constant \"a.\" It is interesting to note that the earliest level of this Herbrand universe $H$ to include such a proof set is $H_5$, which has of the order of $10^64$ members. Consequently $H_5(\{C_1, C_2\})$ has of the order of $10^256$ members. A level-saturation procedure would not find this example feasible.

\begin{enumerate}[label=Example \arabic*.]
    \setcounter{enumi}{1}
    \item A more interesting example is one which was discussed in \cite{robinson_1963}. It arises from the following algebraic problem.
    
    \emph{Prove that in any associative system which has left and right solutions $x$ and $y$ for all equations $x*a = b$ and $a * y = b$, there is a right identity element.}
    
    To formalize this problem i~ our logic, we deny the alleged conclusion, and try to refute the set containing the clauses (where $Q(x,y,z)$ is to mean $x*y = z$):
    
    %TODO fix the associativity brace not spanning C_1 C_2 see page 14
    \begin{align*}
        C_1 : && \{\sim Q(x,y,u), \sim Q(y,z,v), \sim Q(x,v,w), Q(u,z,w)\} && \text{Associativity} \\
        C_2 : && \{\sim Q(x,y,u), \sim Q(y,z,v), \sim Q(u,z,w), Q(x,v,w)\} && \text{Associativity} \\
        C_3 : && \{Q(g(x,y),x,y)\} && \text{Existence of left and right} \\
        C_4 : && \{Q(x, h(x,y), y)\} && \text{solutions} \\
        C_5 : && \{Q(x,y, f(x,y))\} && \text{Closure under *} \\
        C_6 : && \{\sim Q(k(x), x, k(x))\} && \text{No right identity.} \\ \\
        && \text{By adding the following resolvents, we get a refutation:} && \\ \\
        C_7 : && \{\sim Q(y_1, x_6, y_1), Q(y_2, x_6, y_2)\} && \\
        C_8 : && \{\sim Q(y_1, y_2, y_1)\} && \\
        C_9 : && \square &&
    \end{align*}
\end{enumerate}

\emph{Commentary.} $C_7$ is the resolvent of the pair $\langle C_1, C_3\rangle$ for the key triple

\begin{align*}
    \langle\{\sim Q(x,y,u), \sim Q(x,v,w)\}, \{ Q(g(x,y), x, y)\}, N\rangle
\end{align*}
\noindent
where $N$ is the set $\{Q(x_4, x_5, x_1), Q(x_4,x_2,x_3), Q(g(y_1,y_2),y_1,y_2)\}.$ The $\sigma_N$ computed for this $N$ by the Unification Algorithm is

\begin{align*}
    \{y_2/x_1, y_1/x_2, y_2/x_3, g(y_1, y_2)/x_4, y_1/x_5\},
\end{align*}
\noindent
as is easily verified. $C_8$ is the only resolvent of $\langle C_6, C_7\rangle$, and $\square$ is the only resolvent of $\langle C_4, C_8\rangle$.

This example illustrates the way in which the single steps in a refutation made with the resolution principle go beyond, in their complexity, the capacity of the human mind to apprehend their correctness in one single intellectual act. By taking larger bites, so to speak, the resolution principle in this ease permits a very compact, not to say elegant, piece of reasoning. $C_2$ and $C_5$ are not used as premisses in the refutations, although this has nothing to do with the resolution principle. Hence a nonredundant refutation for this example is the sequence: $C_1, C_3, C_4, C_6, C_7, C_8, \square$.

\newpage

\section{Refutation Procedures, Search Principles}

The foregoing discussion was intended only to establish the theoretical framework, in the form of a special system of logic, for the design of theorem-proving programs, i.e. in the present case, refutation procedures. No attempt has been made thus far to discuss the question of developing efficient refutation procedures, and in this final section of the paper we briefly discuss this question. 

The raw implementation of the Resolution Theorem would produce a very 
inefficient refutation procedure, namely, the procedure would consist of computing, given the finite set $S$ of clauses as input, the sequence of sets $S, \mathscr{R}(S), \mathscr{R}^2(S),\dotsm,$ until one is encountered, say, $\mathscr{R}^n(S)$, which either contains $\square$ or else does not contain $\square$ but is equal to its successor $\mathscr{R}^{n+1}(S)$. In the former case, a refutation of $S$ is obtained by tracing back the genesis of $\square$; in the latter case the conclusion is that $S$ is satisfiable. By Church's Theorem \cite{church_1936} we know that for some inputs $S$ this procedure, and in general all correct refutation procedures, will not terminate in either of these two ways but will continue computing indefinitely. 

In some cases we can foresee the nonterminating behavior. Consider the example of the set $S$ whose members are:

\begin{align*}
    C_1 : \{Q(a)\}, && C_2 : \{\sim Q(x), Q(f(x))\}.
\end{align*}
\noindent
(The reader will recognize this as the formulation, in our logic, of a fragment of Peano's postulates for the natural numbers, with "$Q(x)$" for "$x$ is a natural number," "$a$" for "$0$," and "$f(x)$" for "the successor of $x$".) It is easy to see that for this $S$ the procedure described above would generate successively the resolvents\\$\{Q(f(a))\},\{Q(f(f(a)))\},\{Q(f(f(f(a))))\},\dotsm$, etc., \emph{ad infinitum}.

This example suggests out" attempting to formulate a principle which would 
allow us effectively to recognize the particular indefinite continuation phenomenon which it exhibits, so that we might incorporate the principle into a refutation procedure and cause it to terminate for this $S$ and for other similar examples. Such a principle, which we call the \emph{purity principle}, is available, based on the notion of a literal being pure in a set $S$ of clauses. We define this notion as follows.

\subsection{Pure literals.}
If $S$ is any finite set of clauses, $C$ a clause in $S$, and $L$ a literal in $C$ with the property that there is no key triple $\langle\{L\}, M, N\rangle$ for any pair $\langle C,D\rangle$ of clauses, where $D$ is any clause in $S - \{C\}$, then $L$ is said to be pure in $S$.

The purity principle is then based on the following theorem. 

\begin{p15theorem}{Purity Theorem}
If $S$ is any finite set of clauses, and $L\in C\in S$ is a literal which is pure in $S$, then $S$ is satisfiable if and only if $S - \{C\}$ is satisfiable.
\end{p15theorem}
\begin{proof}
If $S$ is satisfiable then so is $S - \{C\}$ since it is a subset of $S$. If $S - \{C\}$
is satisfiable, then there is a model $A$ of $S - \{C\}$, every literal in which occurs 
in some clause of $H(S)$, where $H$ is the Herbrand universe of $S$. Let $N$ be the 
set of all ground literals $L\theta$, where $\theta$ ks a substitution over $H$, and let $K$ consist 
of all complements of members of $N$. Then the set $P = N \cup (A - K)$ is a
\end{proof}

\newpage
model; moreover it is a model $S$, since every clause in $H(\{C\})$ contains a member of $P$ (namely a member of $N$), and every clause in $H(S - \{C\}$ contains a member of $P$, namely a member of $A - K$; for no clause in $H(S - \{C\}$ contains a member of $K$, since otherwise, if $D\beta$ were, such a clause, with $D \in S - \{C\}$, then there would be an $M\subseteq D$ such that $M\beta$ would be a singleton containing a member of $K$. Then there would be some substitution $a$ over $H$ such that $\{L\}\alpha, M\beta$ contained complementary singletons. Hence by the same argument as in the proof of the Lemma, there would be a key triple $\langle\{L\}, M, N\rangle$ 
of the pair $\langle C,D\rangle$, contradicting the purity of $L$ in $S$. The theorem is proved.

The \emph{purity principle} is then simply the following: \emph{One may delete, from a finite set $S$ of clauses, any clause containing a literal which is pure in $S$.}

When $S$ is the little Peano example given earlier, i.e., is the set containing just the two clauses

\begin{align*}
    C_1 : \{Q(a)\}, && C_2 : \{\sim Q(x), \underline{Q(f(x))}\}.
\end{align*}

we see that the underlined literal in $C_2$ is pure in S. Hence we may delete C2, obtaining the set $S - {C_2}$ whose only clause is

\begin{align*}
    C_1 : \{\underline{Q(a)}\}.
\end{align*}

But of course the underlined literal is, trivially, pure in $S - \{C_2\}$; hence we may delete $C_1$, obtaining the set $S - \{C_1\} - \{C_2\}$, which is empty, hence satisfiable. Hence by the Purity Theorem, $S$ is satisfiable. 

Thus a refutation procedure incorporating the purity principle as well as the resolution principle "converges" for more finite sets of clauses than a procedure based on the resolution principle alone. Such principles as the purity principle we call \emph{search principles} to distinguish them from inference principles.

There is another search principle which, though not increasing the range of convergence, does help to increase the rate of convergence, of refutation procedures. We call this principle the \emph{subsumption principle} and base it on the following definition.

\subsection{Subsumption}
If $C$ and $D$ are two distinct nonempty clauses, we say 
that $C$ subsumes $D$ just in case there is a substitution $\sigma$ such that $C\sigma\subseteq D$.

The following theorem establishes the basic property of subsumption. 

\newcustomtheorem{p16theorem}{}
\begin{p16theorem}{Subsumption Theorem}
If $S$ is any finite set of clause. and $D$ is any clause in $S$ which is subsumed by some clause in $S - \{D\}$, then $S$ is satisfiable if and only if $S - \{D\}$ is satisfiable.
\end{p16theorem}
\begin{proof}
We need only show that if $M$ is a model of $S - \{D\}$, then $M$ is a model of $S$. Let $M$ be a model of $S - \{D\}$, and suppose that $C \in S - \{D\}$ subsumes $D$. Then there is a substitution $\sigma$ such that $C\sigma\subseteq D$. Since $D \in S$, the 
terms of the components of $\sigma$ must be formed from function symbols in the functional vocabulary of $S$, together possibly with variables. Hence every ground instance of $C\sigma$ over $H$ is a ground instance of $C$ over $H$, and hence contains a 
member of $M$. But every ground instance $D\lambda$ of $D$ includes the ground instance $C\sigma\lambda$ of $C$, and hence contains a member of $M$. So $M$ is a model of $S$ and the theorem is proved. 

\end{proof}

\newpage
The \emph{subsumption principle} is then simply the following: \emph{One may delete, from a
finite set $S$ of clauses, any clause $D$ which is subsumed by a clause in $S - \{D\}$.}

In order to make the subsumption principle available for incorporation into a 
refutation procedure, we must give an algorithm for deciding whether one clause $C$ 
subsumes another clause $D$. Such an algorithm is following the Subsumption Algorithm:

\begin{steps}
    \item Let $V_1,\dotsm,V_m$ be all the distinct variables, in alphabetical order, of $D$. Let $J_1,\dotsm,J_m$ be distinct individual constants, none of which occur in $C$ or $D$. Let $\theta = \{J_1/V_1,\dotsm,J_m/V_m\}$. Compute $D\theta$ and go to step 2.
    \item Set $A_0 = \{C\}, k = 0$, and go to step 3.
    \item If $A_k$ does not contain $\square$, let $A_{k+1}$ be the set of all clauses of the form $(K\sigma_N - M\sigma_N$, where $K \in A_k, M\subseteq K, N = M \cup \{P\}$, for some $P \in D\theta$, and $N$ is most generally unifiable with most general unifier $\sigma_N$; and go to step 4. Otherwise, terminate.
    \item If $A_{k+1}$ is nonempty, add $1$ to $k$ and return to step 3. Otherwise, terminate.
\end{steps}

That this is an algorithm is clear from the fact that each clause in $A_{k+1}$ is smaller, by at least one literal, than the clause in $A_k$, from which it was obtained. Hence, since the only clause in $A_0$ has but finitely many literals, the sequence $A_0, A_1,\dotsm$, must eventually contain a set which contains $\square$ or is empty. 

That the Subsumption Algorithm is correct is shown by the following argument that it terminates in step 3 if and only if $C$ subsumes $D$.

If $C$ subsumes $D$, then $C\sigma\subseteq D$ for some $\sigma$. Hence $C\sigma\theta\subseteq D\theta$ Hence $C\mu\subseteq D\theta$, for some $\mu$. Now assume, for $k\geqq 0$, that $K \in A_k$ and that, for some $\mu$, $K\mu\subseteq D\theta$. If $K$ is not $\square$, let $P$ be a literal in $K_\mu \cap D\theta$. Then there is an $M \subseteq K$ such that $N = (M\cup\{P\})$ is unified by $\mu$. Therefore by the Unification Theorem $N$ has a most general unifier $\sigma_N$, and the clause $G = (K\sigma_N - M\sigma_N)$ is in $A_{k+1}$ But by the Unification Theorem $\mu = \sigma_N\lambda$, for some $\lambda$, hence $K\sigma_N\lambda\subseteq D\theta$. Therefore $G\lambda\subseteq D\theta$. Since $C \in A_0$, this shows that each $A_k, k\geqq 0$, either contains $\square$ or is nonempty. Hence the Subsumption Algorithm does not terminate in step 4. Therefore it terminates in step 3.

If the Subsumption Algorithm terminates in step 3, for $C$ and $D$ as input, then there is a finite sequence
$C_0,C_1,\dotsm,C_{n+1}$ of clauses such that $C_0 = C, C_{n+1} = \square$, and for $0\leq j \leq n, C_{j+1} = C_{j\sigma j} - M_{j\sigma j}$, where $M_j\subseteq C_j$, and $\sigma_j$ is the most general unifier of $M_j \cup\{P\}$, where $P\in D\theta$. It follows that (since $M_{j\sigma j}$ contains no variables, $0\leq j\leq n$) we have
\begin{align*}
    C_{n+1} = \square = C_{\sigma \theta \sigma 1}\dotsm \sigma_n - M_{0\sigma0} - M_{1\sigma 1} - \dotsm - M_{n\sigma n}
\end{align*}
\noindent
i.e. that $C_{\sigma 0\sigma 1}\dotsm\sigma_n\subseteq(M_{0\sigma 0}\cup M_{1\sigma 1}\cup\dotsm\cup M_{n\sigma n})\subseteq D\theta$. Hence, for some $\lambda, C\lambda\subseteq D\theta$. Let $\sigma$ be the substitution obtained from $\lambda$ by the replacement, in each component of $\lambda$ of $J_i$ by $V_i$, for $1\leq i\leq m$. Then $C\sigma\subseteq D$.

A particularly useful application of the subsumption principle is the following: Suppose a resolvent $R$ of $C$ and $D$ subsumes one of $C$, $D$; then in adding 
$R$ by the resolution principle we may simultaneously delete, by the subsumption principle, that one of $C$, $D$ which $R$ subsumes. This combined operation 
amounts to replacing $C$ or $D$ by $R$; accordingly we name the principle involved the \emph{replacement principle}.

\newpage

The following example, used by Gilmore \cite{gilmore_1960}, Davis-Putnam \cite{davis_1960} and Friedman \cite{friedman_1963}, illustrates the utility of these search principles in speeding up convergence.\\
\noindent
Consider the set S whose members are: 
%TODO align left
\begin{align*}
    & C_1 : && \underbrace{\{P(x_1,x_2)}_{(6)} \\
    & C_2 : && \{\underbrace{\sim P(y_2,f(y_1,y_2)),}_{(1)}, \underbrace{\sim P(f(y_1,y_2),f(y_1,y_2))}_{(2)}, Q(y_1,y_2)\} \\
    & C_3 : && \{\underbrace{\sim P(y_2, f(y_1,y_2))}_{(3)}, \underbrace{\sim P(f(y_1,y_2),f(y_1,y_2))}_{(4)},
        \underbrace{\sim Q(y_1,f(y_1,y_2))}_{(5)}, \sim Q(f(y_1,y_2),f(y_1,y_2))\}
\end{align*}
\noindent
and we obtain the set $S^\prime$ whose only members are:

\begin{align*}
    & C_4 : && \{Q(y_1,y_2)\} \\
    & C_5 : && \{\sim Q(f(y_1, y_2), f(y_1, y_2))\}
\end{align*}
\noindent
in six stages which may be followed through by deleting the underlined literals, and the underlined clause, in the order indicated. This gives the set of clauses at each stage. Deletions (1) through (5) are by virtue of the replacement principle; deletion (6), of the entire clause $C_1$, is by virtue of the purity principle. The set $S^\prime$ in turn is found immediately to be unsatisfiable, since $C_4$ and $C_5$ have $\square$ as their only resolvent.

Gilmore's 704 program failed to converge after 21 minutes' running time, when given this example. The more efficient procedure of Davis and Putnam converges, for this example, in 30 minutes of hand computation.

The application, to a finite set $S$ of clauses, of any of the three search principles we have described, produces a set $S^\prime$ which either has fewer clauses than $S$ or has the same number of clauses as $S$ but with one or more shorter clauses. An obvious method of exploiting these principles in a refutation procedure is therefore never to add new clauses, by the resolution principle, except to a set to which the three principles are no longer applicable. We might call such sets \emph{irreducible}. The way in which such a procedure would terminate, for satisfiable 
$S$ within its range of convergence, would then be with a set, which is either empty (as in the Peano example) or nonempty, irreducible, and with the property that each resolvent of any pair of its clauses is subsumed by some one of its clauses. 

There are further search principles of this same general sort, which are less simple than those discussed in this section. A sequel to the present paper is planned in which the theoretical framework developed here will be used as the basis for a more extensive treatment of search principles and of the design of refutation procedures. This section has been merely a sketch of the general nature of the problem, and a brief view of some of the approaches to if,.

\emph{Acknowledgments}. I should like to express my indebtedness to my colleagues Dr. George A. Robinson and Dr. Lawrence T. Wos, of the Argonne National Laboratory, and to Professor William Davidon of Haverford College, for their

\newpage
invaluable insights and criticisms concerning the basic concepts of this paper. My thanks are also due to the ACM referees, and to Dr. T. Hailperin of the Sandia Corporation, whose comments on criticisms of a prior version of the paper greatly facilitated the writing of the present complete revision.
\noindent
Received September, 1963; Revised August, 1964
\bibliographystyle{acm}
\bibliography{resolution.bib}
\end{document}

%TODO fix all set braces