%http://ropas.snu.ac.kr/lib/dock/Ro1965.pdf
\documentclass[8pt]{extarticle}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}

\geometry{
    paperheight=9.75in,
    paperwidth=6.25in,
    left=0.90in,
    top=0.5in
}

%TODO fix horizontal spacing of title
\title{\textbf{ A Machine-Oriented Logic Based on the Resolution Principle}}
\author{J. A. Robinson}
\date{\emph{Argonne National Laboratory* and Rice University$\dagger$}}

\begin{document}

\maketitle


\textit{Abstract}. Theorem-proving on the computer, using procedures based on the fundamental theorem of Herbrand concerning the first-order predicate calculus, is examined with a view towards improving the efficiency and widening the range of practical applicability of these procedures. A close analysis of the process of substitution (of terms for variables), and the process of truth-functional analysis of the results of such substitutions, reveals that both processes can be combined into a single new process (called \emph{resolution}), iterating which is vastly more efficient than the older cyclic procedures consisting of substitution stages alternating with truth-functional analysis stages.

The theory of the resolution process is presented in the form of a system of first-order logic with just one inference principle (the resolution principle). The completeness of the system is proved; the simplest proof-procedure based on the system is then the direct implementation of the proof of completeness. However, this procedure is quite inefficient, and the paper concludes with a discussion of several principles (called search principles) which are applicable to the design of efficient proof-procedures employing resolution as the
basic logical process.

\section{Introduction}
Presented in this paper is a formulation of first-order logic which is specifically designed for use as the basic theoretical instrument of a computer theorem-proving program. Earlier theorem-proving programs have been based on systems of first-order logic which were originally devised for other purposes. A prominent feature of those systems of logic, which is lacking in the system described in this paper, is the relative \emph{simplicity} of their inference principles.

Traditionally, a single step in a deduction has been required, for pragmatic and psychological reasons, to be simple enough, broadly speaking, to be apprehended as correct by a human being in a single intellectual act. No doubt this custom originates in the desire that each single step of a deduction should be indubitable, even though the deduction as a whole may consist of a long chain of such steps. The ultimate conclusion of a deduction, if the deduction is correct, follows logically from the premisses used in the  deduction; but the human mind may well find the unmediated transition from the premisses to the conclusion surprising, hence (psychologically) dubitable. Part of the point, then, of the logical analysis of deductive reasoning has been to reduce complex inferences, which are beyond the capacity of the human mind to grasp as single steps, to chains of simpler inferences, each of which is within the capacity of the human mind to grasp as a
single transaction.\\

Work performed under the auspices of the U. S. Atomic Energy Commission.\\

* Argonne, Illinois.

$\dagger$ Present address: Rice University, Houston, Texas. \\

%TODO fix firstpage footer
Journal of the Association for Computing Machinery, Val. 1 2, No. 1 (January, 1965), pp. 23-41

\newpage

From the theoretical point of view, however, an inference principle need only be \emph{sound} (i.e., allow only logical consequences of premisses to be deduced from them) and \emph{effective} (i.e., it must be algorithmically decidable whether an alleged application of the interference principle is indeed an application of it). When the agent carrying out the application of an inference principle is a modern computing machine, the traditional limitation on the complexity of inference principles is no longer very appropriate. More powerful principles, involving perhaps a much greater amount of combinatorial information-processing for a single application, become a possibility.

In the system described in this paper, one such inference principle is used. It is called the \emph{resolution principle}, and it is machine-oriented, rather than human-oriented, in the sense of the preceding remarks. The resolution principle is quite powerful, both in the psychological sense that it condones single inferences which are often beyond the ability of the human to grasp (other than discursively), and in the theoretical sense that it alone, as sole inference principle, forms a complete system of first-order logic. While this latter property is of no great importance, it is interesting that (as far as the author is aware) no other complete system of first-order logic has consisted of just one inference principle, if one construes the device of introducing a logical axiom, given outright, or by a schema, as a (premiss-free) inference principle.

The main advantage of the resolution principle lies in its ability to allow us to avoid one of the major combinatorial obstacles to efficiency which have plagued earlier theorem-proving procedures.

In Section 2 the syntax and semantics of the particular formalism which is used in this paper are explained.

\section{Formal Preliminaries}

The formalism used in this paper is based upon the notions of unsatisfiability and refutation rather than upon the notions of validity and proof. It is well known (cf. \cite{davis_1960} and \cite{robinson_1963}) that in order to determine whether a finite set $S$ of sentences of first-order logic is satisfiable, it is sufficient to assume that each sentence in $S$ is in prenex form with no existential quantifiers in the prefix; moreover the matrix of each sentence in $S$ can be assumed to be a disjunction of formulas each of which is either an atomic formula or the negation of an atomic formula. Therefore our syntax is set up so that the natural syntactical unit is a finite set $S$ of sentences in this special form. The quantifier prefix is omitted from each sentence, since it consists just of universal quantifiers binding each variable in the sentence; furthermore the matrix of each sentence is regarded simple as the set of its disjuncts, on the grounds that the order and multiplicity of the disjuncts in a disjunction are immaterial.

Accordingly we introduce the following definitions (following in part the nomenclature of \cite{davis_1960} and \cite{robinson_1963}):\\
2.1 \emph{Variables}. The following symbols, in alphabetical order, are variables:
\begin{align*}
    u, v, w, x, y, z, u_1 , v_1 , w_1 , x_1 , y_1 , z_1 , u_2 , \dotsm , etc.
\end{align*}
\newpage

2.2 \emph{Function symbols}. The following symbols, in alphabetical order, are function symbols of degree $n$, for each $n\geqq0$:
\begin{align*}
    a^n,b^n,c^n,d^n,e^n,f^n,g^n,h^n,k^n,a_1^n,b_1^n,\dotsm, etc.
\end{align*}

\noindent When $n = 0$, the superscript may be omitted. Function symbols of degree 0 are \emph{individual constants}.

2.3 \emph{Predicate symbols}. The following symbols, in alphabetical order, are predicate symbols of degree $n$, for each $n\geqq0$:
\begin{align*}
    P^n,Q^n,R^n,P_1^n,Q_1^n,R_1^n,P_2^n,\dotsm, etc.
\end{align*}

\noindent The superscript may be omitted when $n$ is 0.

2.4 \emph{The negation symbol}. The following symbol is the negation symbol: $\sim$

2.5 \emph{Alphabetical order of symbols}. The set of all symbols is well ordered in alphabetical order by adding to the above ordering conventions the rule that variables precede function symbols, function symbols precede predicate symbols, predicate symbols precede the negation symbol, function symbols of lower degree precede function symbols of higher degree, and predicate symbols of lower degree precede predicate symbols of higher degree.

2.6 \emph{Terms}. A variable is a term, and a string of symbols consisting of a function symbol of degree $n\geqq0$ followed by $n$ terms is a term.

2.7 \emph{Atomic formulas}. A string of symbols consisting of a predicate symbol of degree $n\geqq0$ followed by $n$ terms is an atomic formula.

2.8 \emph{Literals}. An atomic formula is a literal; and if $A$ is an atomic formula then $\sim A$ is a literal.

2.9 \emph{Complements}. If $A$ is an atomic formula, then the two literals $A$ and $\sim A$ are said to be each other's complements, and to form, in either order, a complementary pair.

2.10 \emph{Clauses}. A finite set (possible empty) of literals is called a clause. The empty clause is denoted by: $\square$

2.11 \emph{Ground literals}. A literal which contains no variables is called a ground literal.

2.12 \emph{Ground clauses}. A clause, each member of which is a ground literal, is called a ground clause. In particular $\square$ is a ground clause.

2.13 \emph{Well-formed expressions}. Terms and literals are (the only) well formed expressions.

2.14 \emph{Lexical order of well-formed expressions}. The set of all well formed expressions is well ordered in lexical order by the rule that $A$ precedes $B$ just in case that $A$ is shorter than $B$ or, if $A$ and $B$ are of equal length, then $A$ has the alphabetically earlier symbol in the first symbol position at which $A$ and $B$ have distinct symbols.

In writing well-formed expressions for illustrative purposes, we follow the more readable plan of enclosing the $n$ terms following a function symbol or predicate symbol of degree $n$ by a pair of parentheses, separating the terms, if there are two or more, by commas. We can then unambiguously omit all superscripts from symbols. In writing finite sets, we follow the usual convention of

\newpage
\noindent enclosing the members in a pair of braces and of separating the members by commas, with the understanding that the order of writing the members in immaterial.

2.15 \emph{Herbrand universes}. With any set $S$ of clauses there is associated a set of ground terms called the Herbrand universe of $S$, as follows: let $F$ be the set of all function symbols which occur in $S$. If $F$ contains any function symbols of degree 0, the functional vocabulary of $S$ is $F$; otherwise it is the set ${a}\union F$. The Herbrand universe of $S$ is then the set of all ground terms in which there occur only symbols in the functional vocabulary of $S$.

2.16 \emph{Saturation}. If $S$ is any set of clauses and $P$ is any set of terms, then by $P(S)$ we denote the saturation of $S$ over $P$, which is the set of all ground clauses obtainable from members of $S$ by replacing variables with members of P-occurrences of the same variable in any one clause being replaced by occurrences of the same term.

2.17 \emph{Models}. A set of ground literals which does not include a complementary pair is called a model. If $M$ is a model and $S$ is a set of ground clauses, then $M$ is a model of $S$ if, for all $C$ in $S$, $C$ contains a member of $M$. Then, in general, if $S$ is any set of clauses, and $H$ is the Herbrand universe of $S$, we say that $M$ is a model of $S$ just in case that $M$ is a model of $H(S)$.

2.18 \emph{Satisfiability}. A set $S$ of clauses is satisfiable if there is a model of $S$; otherwise $S$ is unsatisfiable.

From the definition of satisfiability, it is clear that any set of clauses which contain $\square$ is unsatisfiable, and that the empty set of clauses is satisfiable. These two circumstances will appear quite natural as the development of our system proceeds. It is also clear that according to our semantic definitions each nonempty clause is interpreted, as explained in the informal remarks at the beginning of this section, as the universal closure of the disjunction of the literal which it contains.

2.19 \emph{Ground resolvents}. If $C$ and $D$ are two ground clauses, and $L\subseteq C,M\subseteq D$ are two singletons (unit sets) whose respective members form a complementary pair, then the ground clause: $(C - L)\cup (D - M)$ is called a ground resolvent of $C$ and $D$.

Evidently any model of ${C,D}$ is also a model of ${C,D,R}$, where $R$ is a ground resolvent of $C$ and $D$. Not all pairs of ground clauses have ground resolvents, and some have more than one; but in no case, as is clear from the definitions, can two ground clauses have more than a finite number of ground resolvents.

2.20 \emph{Ground resolution}. If $S$ is any set of ground clauses, then the ground resolution of $S$, denoted by $\mathscr{R}(S)$, is the set of ground clauses consisting of the members of $S$ together with all ground resolvents of all pairs of members of $S$.

2.21 \emph{N-th ground resolution}. If $S$ is any set of ground clauses, then the $n$th ground resolution of $S$, denoted by $\mathscr{R}^n(S)$, is defined for each $n\geqq 0$ as follows: $\mathscr{R}^0(S) = S$; and for $n\geqq 0$, $\mathscr{R}^{n+1} = \mathscr{R}(\mathscr{R}^n(S))$.

This completes the first batch of definitions. The next sections are concerned with the various forms that Herbrand's Theorem takes on in our system. To each such form, there is a type of refutation procedure which that form sug-

\newpage

\newpage

TODO REMOVE
\cite{church_1936}
\cite{davis_1960}
\cite{friedman_1963}
\cite{gilmore_1960}
\cite{robinson_1963}

\bibliographystyle{acm}
\bibliography{resolution.bib}
\end{document}
