%http://ropas.snu.ac.kr/lib/dock/Ro1965.pdf
\documentclass[8pt]{extarticle}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{titlesec}
\usepackage{enumitem}
\newlist{steps}{enumerate}{1}
\setlist[steps, 1]{label = Step \arabic*.}

%https://tex.stackexchange.com/a/164334
%TODO style subsection's more
\titleformat{\subsection}[runin]
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}[runin]
  {\normalfont\large\bfseries}{\thesubsubsection}{1em}{}

\geometry{
    paperheight=9.75in,
    paperwidth=6.25in,
    left=0.90in,
    top=0.5in
}

%https://tex.stackexchange.com/a/53981
\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{p5herbrandt}{}
\newcustomtheorem{p5theorem}{Theorem}       %Page 5 Theorem 1
\newcustomtheorem{grt}{}                    %Page 6 Ground Resolution Theorem
\newcustomtheorem{p7theorem}{Theorem}       %Page 7 Theorem 2
\newcustomtheorem{p7lemma}{}           %Page 7 Lemma
\newcustomtheorem{p7corollary}{Corollary}   %Page 7 Corollary
\newcustomtheorem{pg8theorem3}{Theorem}     %Page 8 Theorem 3
\newcustomtheorem{pg8rt}{Resolution Theorem}%Page 8 Resolution Theorem

%TODO fix horizontal spacing of title
\title{\textbf{ A Machine-Oriented Logic Based on the Resolution Principle}}
\author{J. A. Robinson}
\date{\emph{Argonne National Laboratory* and Rice University$\dagger$}}

\begin{document}

\maketitle


\textit{Abstract}. Theorem-proving on the computer, using procedures based on the fundamental theorem of Herbrand concerning the first-order predicate calculus, is examined with a view towards improving the efficiency and widening the range of practical applicability of these procedures. A close analysis of the process of substitution (of terms for variables), and the process of truth-functional analysis of the results of such substitutions, reveals that both processes can be combined into a single new process (called \emph{resolution}), iterating which is vastly more efficient than the older cyclic procedures consisting of substitution stages alternating with truth-functional analysis stages.

The theory of the resolution process is presented in the form of a system of first-order logic with just one inference principle (the resolution principle). The completeness of the system is proved; the simplest proof-procedure based on the system is then the direct implementation of the proof of completeness. However, this procedure is quite inefficient, and the paper concludes with a discussion of several principles (called search principles) which are applicable to the design of efficient proof-procedures employing resolution as the
basic logical process.

\section{Introduction}
Presented in this paper is a formulation of first-order logic which is specifically designed for use as the basic theoretical instrument of a computer theorem-proving program. Earlier theorem-proving programs have been based on systems of first-order logic which were originally devised for other purposes. A prominent feature of those systems of logic, which is lacking in the system described in this paper, is the relative \emph{simplicity} of their inference principles.

Traditionally, a single step in a deduction has been required, for pragmatic and psychological reasons, to be simple enough, broadly speaking, to be apprehended as correct by a human being in a single intellectual act. No doubt this custom originates in the desire that each single step of a deduction should be indubitable, even though the deduction as a whole may consist of a long chain of such steps. The ultimate conclusion of a deduction, if the deduction is correct, follows logically from the premisses used in the  deduction; but the human mind may well find the unmediated transition from the premisses to the conclusion surprising, hence (psychologically) dubitable. Part of the point, then, of the logical analysis of deductive reasoning has been to reduce complex inferences, which are beyond the capacity of the human mind to grasp as single steps, to chains of simpler inferences, each of which is within the capacity of the human mind to grasp as a
single transaction.
\vspace{0.3cm}

Work performed under the auspices of the U. S. Atomic Energy Commission.

* Argonne, Illinois.

$\dagger$ Present address: Rice University, Houston, Texas.

\vspace{0.2in}
%TODO fix firstpage footer
Journal of the Association for Computing Machinery, Val. 1 2, No. 1 (January, 1965), pp. 23-41

\newpage

From the theoretical point of view, however, an inference principle need only be \emph{sound} (i.e., allow only logical consequences of premisses to be deduced from them) and \emph{effective} (i.e., it must be algorithmically decidable whether an alleged application of the interference principle is indeed an application of it). When the agent carrying out the application of an inference principle is a modern computing machine, the traditional limitation on the complexity of inference principles is no longer very appropriate. More powerful principles, involving perhaps a much greater amount of combinatorial information-processing for a single application, become a possibility.

In the system described in this paper, one such inference principle is used. It is called the \emph{resolution principle}, and it is machine-oriented, rather than human-oriented, in the sense of the preceding remarks. The resolution principle is quite powerful, both in the psychological sense that it condones single inferences which are often beyond the ability of the human to grasp (other than discursively), and in the theoretical sense that it alone, as sole inference principle, forms a complete system of first-order logic. While this latter property is of no great importance, it is interesting that (as far as the author is aware) no other complete system of first-order logic has consisted of just one inference principle, if one construes the device of introducing a logical axiom, given outright, or by a schema, as a (premiss-free) inference principle.

The main advantage of the resolution principle lies in its ability to allow us to avoid one of the major combinatorial obstacles to efficiency which have plagued earlier theorem-proving procedures.

In Section 2 the syntax and semantics of the particular formalism which is used in this paper are explained.

\section{Formal Preliminaries}

The formalism used in this paper is based upon the notions of unsatisfiability and refutation rather than upon the notions of validity and proof. It is well known (cf. \cite{davis_1960} and \cite{robinson_1963}) that in order to determine whether a finite set $S$ of sentences of first-order logic is satisfiable, it is sufficient to assume that each sentence in $S$ is in prenex form with no existential quantifiers in the prefix; moreover the matrix of each sentence in $S$ can be assumed to be a disjunction of formulas each of which is either an atomic formula or the negation of an atomic formula. Therefore our syntax is set up so that the natural syntactical unit is a finite set $S$ of sentences in this special form. The quantifier prefix is omitted from each sentence, since it consists just of universal quantifiers binding each variable in the sentence; furthermore the matrix of each sentence is regarded simple as the set of its disjuncts, on the grounds that the order and multiplicity of the disjuncts in a disjunction are immaterial.

Accordingly we introduce the following definitions (following in part the nomenclature of \cite{davis_1960} and \cite{robinson_1963}):\\
\subsection{Variables}. The following symbols, in alphabetical order, are variables:
\begin{align*}
    u, v, w, x, y, z, u_1 , v_1 , w_1 , x_1 , y_1 , z_1 , u_2 , \dotsm , etc.
\end{align*}
\newpage

\subsection{Function symbols}. The following symbols, in alphabetical order, are function symbols of degree $n$, for each $n\geqq0$:
\begin{align*}
    a^n,b^n,c^n,d^n,e^n,f^n,g^n,h^n,k^n,a_1^n,b_1^n,\dotsm, etc.
\end{align*}

\noindent When $n = 0$, the superscript may be omitted. Function symbols of degree 0 are \emph{individual constants}.

\subsection{Predicate symbols}. The following symbols, in alphabetical order, are predicate symbols of degree $n$, for each $n\geqq0$:
\begin{align*}
    P^n,Q^n,R^n,P_1^n,Q_1^n,R_1^n,P_2^n,\dotsm, etc.
\end{align*}

\noindent The superscript may be omitted when $n$ is 0.

\subsection{The negation symbol}. The following symbol is the negation symbol: $\sim$

\subsection{Alphabetical order of symbols}. The set of all symbols is well ordered in alphabetical order by adding to the above ordering conventions the rule that variables precede function symbols, function symbols precede predicate symbols, predicate symbols precede the negation symbol, function symbols of lower degree precede function symbols of higher degree, and predicate symbols of lower degree precede predicate symbols of higher degree.

\subsection{Terms}. A variable is a term, and a string of symbols consisting of a function symbol of degree $n\geqq0$ followed by $n$ terms is a term.

\subsection{Atomic formulas}. A string of symbols consisting of a predicate symbol of degree $n\geqq0$ followed by $n$ terms is an atomic formula.

\subsection{Literals}. An atomic formula is a literal; and if $A$ is an atomic formula then $\sim A$ is a literal.

\subsection{Complements}. If $A$ is an atomic formula, then the two literals $A$ and $\sim A$ are said to be each other's complements, and to form, in either order, a complementary pair.

\subsection{Clauses}. A finite set (possible empty) of literals is called a clause. The empty clause is denoted by: $\square$

\subsection{Ground literals}. A literal which contains no variables is called a ground literal.

\subsection{Ground clauses}. A clause, each member of which is a ground literal, is called a ground clause. In particular $\square$ is a ground clause.

\subsection{Well-formed expressions}. Terms and literals are (the only) well formed expressions.

\subsection{Lexical order of well-formed expressions}. The set of all well formed expressions is well ordered in lexical order by the rule that $A$ precedes $B$ just in case that $A$ is shorter than $B$ or, if $A$ and $B$ are of equal length, then $A$ has the alphabetically earlier symbol in the first symbol position at which $A$ and $B$ have distinct symbols.

In writing well-formed expressions for illustrative purposes, we follow the more readable plan of enclosing the $n$ terms following a function symbol or predicate symbol of degree $n$ by a pair of parentheses, separating the terms, if there are two or more, by commas. We can then unambiguously omit all superscripts from symbols. In writing finite sets, we follow the usual convention of

\newpage
\noindent enclosing the members in a pair of braces and of separating the members by commas, with the understanding that the order of writing the members in immaterial.

\subsection{Herbrand universes}. With any set $S$ of clauses there is associated a set of ground terms called the Herbrand universe of $S$, as follows: let $F$ be the set of all function symbols which occur in $S$. If $F$ contains any function symbols of degree 0, the functional vocabulary of $S$ is $F$; otherwise it is the set ${a}\cup F$. The Herbrand universe of $S$ is then the set of all ground terms in which there occur only symbols in the functional vocabulary of $S$.

\subsection{Saturation}. If $S$ is any set of clauses and $P$ is any set of terms, then by $P(S)$ we denote the saturation of $S$ over $P$, which is the set of all ground clauses obtainable from members of $S$ by replacing variables with members of P-occurrences of the same variable in any one clause being replaced by occurrences of the same term.

\subsection{Models}. A set of ground literals which does not include a complementary pair is called a model. If $M$ is a model and $S$ is a set of ground clauses, then $M$ is a model of $S$ if, for all $C$ in $S$, $C$ contains a member of $M$. Then, in general, if $S$ is any set of clauses, and $H$ is the Herbrand universe of $S$, we say that $M$ is a model of $S$ just in case that $M$ is a model of $H(S)$.

\subsection{Satisfiability}. A set $S$ of clauses is satisfiable if there is a model of $S$; otherwise $S$ is unsatisfiable.

From the definition of satisfiability, it is clear that any set of clauses which contain $\square$ is unsatisfiable, and that the empty set of clauses is satisfiable. These two circumstances will appear quite natural as the development of our system proceeds. It is also clear that according to our semantic definitions each nonempty clause is interpreted, as explained in the informal remarks at the beginning of this section, as the universal closure of the disjunction of the literal which it contains.

\subsection{Ground resolvents}. If $C$ and $D$ are two ground clauses, and $L\subseteq C,M\subseteq D$ are two singletons (unit sets) whose respective members form a complementary pair, then the ground clause: $(C - L)\cup (D - M)$ is called a ground resolvent of $C$ and $D$.

Evidently any model of ${C,D}$ is also a model of ${C,D,R}$, where $R$ is a ground resolvent of $C$ and $D$. Not all pairs of ground clauses have ground resolvents, and some have more than one; but in no case, as is clear from the definitions, can two ground clauses have more than a finite number of ground resolvents.

\subsection{Ground resolution}. If $S$ is any set of ground clauses, then the ground resolution of $S$, denoted by $\mathscr{R}(S)$, is the set of ground clauses consisting of the members of $S$ together with all ground resolvents of all pairs of members of $S$.

\subsection{$N$-th ground resolution}. If $S$ is any set of ground clauses, then the $n$th ground resolution of $S$, denoted by $\mathscr{R}^n(S)$, is defined for each $n\geqq 0$ as follows: $\mathscr{R}^0(S) = S$; and for $n\geqq 0$, $\mathscr{R}^{n+1} = \mathscr{R}(\mathscr{R}^n(S))$.

This completes the first batch of definitions. The next sections are concerned with the various forms that Herbrand's Theorem takes on in our system. To each such form, there is a type of refutation procedure which that form suggests

\newpage

\noindent and justifies. The basic version is stated as follows (cf. \cite{davis_1960,gilmore_1960}):

\begin{p5herbrandt}{Herbrand's Theorem}\label{p5herbrandt}
If $S$ is any finite set of clauses and $H$ its Herbrand universe, then $S$ is unsatisfiable if and only if some finite subset of $H(S)$ is unsatisfiable.
\end{p5herbrandt}

\section{\emph{Saturation Procedures}}

Is was noted in an earlier paper \cite{robinson_1963} that one can express Herbrand's Theorem in the following form:

\begin{p5theorem}{1}\label{p5theorem}
If S is any finite set of clauses, then S Ls unsatisfiable if and only if, for some finite subset P of the Herbrand universe of S, P(S) is unsatisfiable. 
\end{p5theorem}

This version of Herbrand's Theorem suggests the following sort of refutation procedure, which we call a \emph{saturation procedure}: given a finite set $S$ of clauses, select a sequence $P_0, P_1, P_2, \dotsm ,$ of finite subsets of the Herbrand universe $H$ of $S$, such that $P_j \subseteq P_{j+1}$ for each $j\geqq$, and such that  $\cup_{j=0}^\infty P_j = H$. Then examine in turn the sets $P_0(S),P_1(S),P_2(S), \dotsm ,$ for satisfiability. Evidently, for any finite subset $P$ of $H$, $P \subseteq P_j$ for some $j$, and therefore $P(S) \subseteq P_j(S)$. Therefore, by Theorem 1, if $S$ is unsatisfiable then, for some $j$, $P_j(S)$ is unsatisfiable.

Of course, any specific procedure of this sort must make the selection of \\$P_0, P_1, P_2,\dotsm$, uniformly for all finite sets of clauses. A  particularly natural way of doing this is to use the so-called levels $H_0, H_1, H2,\dotsm$, of the Herbrand universe $H$; where $H_0$ consists of all the individual constants in $H$, and $H_{n+1}$, for $n\geqq0$, consists of all the terms in $H$ which are in $H_n$, or whose arguments are in $H_n$. In \cite{robinson_1963} we called procedures using this method \emph{level-saturation procedures}. It, was there remarked that essentially the procedures of Gilmore \cite{gilmore_1960} and Davis Putnam \cite{davis_1960} are level-saturation procedures. 

The major combinatorial obstacle to efficiency for level-saturation procedures is the enormous rate of growth of the finite sets $H_j$ and $H_j(S)$ as $j$ increases, at least for most interesting sets $S$. These growth rates were analyzed in some detail in \cite{robinson_1963}, and some examples were there given of some quite simple unsatisfiable S for which the earliest unsatisfiable  $H_j(S)$ is so large as to be absolutely beyond the limits of feasibility.

An interesting heuristic remark is that, for every finite set $S$ of clauses which is unsatisfiable and which has a refutation one could possibly construct, there is at least one reasonably small finite subset of the Herbrand universe of $S$ such that $P(S)$ is unsatisfiable and such that $P$ is \emph{minimal} in the sense that $Q(S)$ is satisfiable for each proper subset $Q$ of $P$. Such a $P$ was called a \emph{proof set} for $S$ in \cite{robinson_1963}. If only, then, a benevolent and omniscient demon were available who could provide us, in reasonable time, with a proof set $P$ for each unsatisfiable finite set $S$ of clauses that we considered, we could simply arrange to saturate $S$ over $P$ and then extract a suitable refutation of $S$ from the resulting finite unsatisfiable set $P(S)$ of ground clauses. This was in fact the underlying scheme of a computer program reported in \cite{robinson_1963}, in which the part of the demon is played, as best his ingenuity allows, by the mathematician using the program. What is really

\newpage

wanted, to be sure, is a simulation of the proof set demon on the computer; but this would appear, intuitively, to be out of the question. 

It turns out that it, is not completely out of the question. In fact, the method developed in the remainder of this paper seems to come quite close to supplying the required demon as a computing process. In Section 4 we take the first major step towards the development of this method by proving more versions of Herbrand's Theorem. We also give a preliminary informal account of the rest of the argument, pending a rigorous treatment in succeeding sections.

\section{The Resolution Theorems and the Basic Lemma}

As a specific method for testing a finite set of ground clauses for satisfiability,
the method of Davis-Putnam \cite{davis_1960} would be hard to improve on from the point
of view of efficiency. However, we now give another method, far less efficient
than theirs, which plays only a theoretical role in our development, and which
is much simpler to state: given the finite set $S$ of ground clauses, form successively
the sets $S, \mathscr{R}(S), \mathscr{R}^2(S),\dotsm $, until either some $\mathscr{R}^n(S)$ contains $\square$, or does not
contain $\square$ but is equal to $\mathscr{R}^{n+1}(S)$. In the former case, $S$ is unsatisfiable; in the
latter case, $S$ is satisfiable. One or other of these two terminating conditions
must eventually occur, since the number of distinct clauses formable from the
finite set of literals which occur in $S$ is finite, and hence in the nested infinite
sequence:

\subsection{} $S \subseteq \mathscr{R}(S) \subseteq \mathscr{R}^2(S) \subseteq \dotsm \subseteq \mathscr{R}^n(S) \subseteq \dotsm$,\\not all of the inclusions are proper, since resolution introduces no new literals.

In view of the finite termination of the described process we can prove its
correctness, as stated above, in the form of the ground resolution theorem. 


\begin{grt}{Ground Resolution Theorem}\label{grt}
If $S$ is any finite set of ground clauses, then $S$ is unsatisfiable if and only if $\mathscr{R}^n(S)$ contains $\square$, for some $n\geqq O$. 
\end{grt}

PROOF. The "if" part is immediate. To prove the "only if" part, let $T$ be the
terminating set $\mathscr{R}^n(S)$ in the sequence (4.1) above, so that $T$ is closed under ground resolution. We need only show that if $T$ does not contain $\square$, then $T$ is satisfiable, and hence $S$ is satisfiable since $S$ $\subseteq$ $T$. Let $L_1, \dotsm , L_k$ be all the distinct atomic formulas which occur in $T$ or whose complements occur in $T$. Let $M$ be the model defined as follows: $M_0$ is the empty set; and for $0 < j \leq k$, $M_j$ is the set $M_{j-1} \cup {L_j}$, unless some clause in $T$ consists entirely of complements of literals in the set $M{j_1}\cup{\sim L_j}$. in which case $M_j$ is the set $M_{j-1}\cup{\sim L_j}$. Finally, $M$ is $M_k$. Now if $T$ does not contain $\square$, $M$ satisfies $T$. For otherwise there is a least $j$, $0 < j \leq k$, such that some clause (say, $C$) in $T$ consists entirely of complements of literals in the set $M_j$. By the definition of $M_j$, therefore, $M_j$ is $M_{j-1} \cup {\sim L_j}$. Hence by the leastness of $j$, $C$ contains $L_j$. But since $M_j$ is $M_{j-1}\cup {\sim L_j}$, there is some clause (say, $D$) in $T$ which consists entirely of complements of literals in the set $M{j_1}\cup {L_j}$. Hence by the leastness of $j$, $D$ contains $\sim L_j$. Then the clause $B = (C - {L_j}) \cup (D - {\sim L_j})$ consists entirely of complements of literals in the set $M_{j-1}$ , unless $B$ is $\square$. But $B$ is a

\newpage

ground resolvent of $C$ and $D$, hence is in $T$, hence is not $\square$. Thus the leastness of $j$ is contradicted and the theorem is proved.

The Ground Resolution Theorem now allows us to state a more specific form
of Theorem 1, namely,


\begin{p7theorem}{2}\label{two}
If $S$ is any finite set of clauses, then $S$ is unsatisfiable if and only
if, jut some finite subset $P$ of the Herbrand universe of $S$, and some $n \geqq O$, $\mathscr{R}^n(P(S))$ contains $\square$.
\end{p7theorem}

It is now possible to state informally the essential steps of the remaining part of the development. We are going to generalize the notions of ground resolvent and ground resolution, respectively, to the notions of resolvent and resolution. by removing the restriction that the clauses involved be only ground clauses. Any two clauses will then have zero, one or more clauses as their resolvents, but in no case more than finitely many. In the special ease that $C$ and $D$ are ground clauses, their resolvents, if any, are precisely their ground resolvents as already defined. Similarly, the notations $\mathscr{R}(S), \mathscr{R}(S)$ will be retained, with $S$ allowed to be any set of clauses. $\mathscr{R}(S)$ will then denote the resolution of $S$, which
is the set of clauses consisting of all members of $S$ together with all resolvents of all pairs of members of $S$. Again, $\mathscr{R}(S)$ is precisely the ground resolution of $S$, already defined, whenever $S$ happens to be a set of ground clauses. 

The details of how this generalization is done must await the formal definitions in Section 5. However, an informal grasp of the general notion of resolution is obtainable now, prior to its exact treatment, from simply contemplating the fundamental property which it will be shown to possess: \emph{resolution is semicommutative with saturation.} More exactly, this property is as stated in the following basic Lemma, which is proved in Section 5:

\begin{p7lemma}{Lemma}\label{p7lemma}
If $S$ is any set of clauses, and $P$ is any subset of the Herbrand universe of $S$, then: $\mathscr{R}(P(S)) \subseteq P(\mathscr{R}(S))$.
\end{p7lemma}

The fact is, as will be shown here, that any ground clause which can be obtained by \emph{first} instantiating over P a pair $C$, $D$ of clauses in $S$, and \emph{then} forming a ground resolvent of the two resulting instances, cast also be obtained by instantiating over $P$ one of the finitely many resolvents of $C$ and $D$.

It is an easy corollary of the basic Lemma that nth resolutions are also semicommutative with saturation: 

\begin{p7corollary}{}\label{p7corollary}
If $S$ is any set of clauses and $P$ is any subset of the Herbrand
universe of $S$, then: $\mathscr{R}^n(P(S)) \subseteq P(\mathscr{R}^n(S))$ for all $n \geqq O$.
\end{p7corollary}

%https://tex.stackexchange.com/a/27284
\begin{proof}
    By induction on $n$. $\mathscr{R}^0(P(S)) = P(S) = P(\mathscr{R}^0(S))$, so that the case $n = 0$ is trivial. And if, for $n \geqq 0$, $\mathscr{R^n}(P(S)) \subseteq P(\mathscr{R}^n(S))$, then:
    \begin{align*}
        \mathscr{R}^{n+1}(P(S)) &= \mathscr{R}(\mathscr{R}^n(P(S))) && \text{by definition of $\mathscr{R}^{n+1}$}\\
                                &\subseteq \mathscr{R}(P(\mathscr{R}^n(S))) && \text{by the induction hypothesis, as $\mathscr{R}$ preserves inclusion,}\\
                                &\subseteq P(\mathscr{R}(\mathscr{R}^n(S))) && \text{by the Lemma,}\\
                                &= P(\mathscr{R}^{n+1}(S)) && \text{by definition of $\mathscr{R}^{n+1}$,}
    \end{align*}
\end{proof}
\noindent and the Corollary is proved.
\newpage

Now by the above Corollary to the basic Lemma we may immediately obtain
a third version, of Herbrand's Theorem from Theorem 2: 

\begin{pg8theorem3}{3}\label{pg8theorem3}
If $S$ is any finite set of clauses, then $S$ is unsatisfiable if and only if, for soem finite subset $P$ of the Herbrand universe of $S$, and some $n\geqq 0$, $P\mathscr{R}^n(S)$ contains $\square$.
\end{pg8theorem3}

Here, the order of the saturation and $n$th resolution operations is reversed.
Now a rather surprising simplification of Theorem 3 can be made, on the basis
of the remark that mere replacement of variables by terms cannot produce $\square$
from a nonempty clause. Hence $P(\mathscr{R}^n(S))$ will contain $\square$ if and only if $\mathscr{R}^n(S)$
contains $\square$. From Theorem 3, therefore, we immediately obtain our final version
of Herbrand's Theorem, which is the main result of this paper, and which we
call:


\begin{pg8rt}
If $S$ is any finite set of clauses, then S is unsatisfiable
if and only if $\mathscr{R}^n(S)$ contains $\square$, for some $n\geqq 0$. 
\end{pg8rt}

The statement of the Resolution Theorem is just that of the Ground Resolution Theorem with the word "ground" omitted. Apart, therefore, from the somewhat more complex way in which the resolvents of two clauses are computed
(described in Section 5) the method suggested by the Resolution Theorem for
testing a finite set S of clauses for unsatisfiability is exactly like that given earlier for the ease that S is a set of ground clauses, and indeed it. automatically reverts to that method when it is applied to a finite set of ground clauses. However, it is no longer the ease in general that the nested sequence

\begin{align*}
    S \subseteq \mathscr{R}(S) \subseteq \mathscr{R}^2(S) \subseteq \dotsm \subseteq \mathscr{R}^n(S) \subseteq \dotsm
\end{align*}

must terminate for all finite $S$. By Church's Theorem this could not be so, for otherwise we would have a decision procedure for satisfiability for our formulation of first-order logic. 

Consider now the "proof set demon" discussed in Section 3. We there supposed that if see were given a proof set $P$ for an unsatisfiable set $S$ of clauses, all we would have to do would be to compute until we encountered the first $\mathscr{R}^n(P(S))$ which contains $\square$, in order to obtain from it a formal refutation of $S$. But the Resolution Theorem assures us that by the time we had computed ,$\mathscr{R}^n(S)$, if not before, we would have turned up $\square$, despite our ignorance of $P$. In this sense the Resolution Theorem makes the proof set demon's role unnecessary.

In Section 5 we introduce a little more formal apparatus by a second batch of definitions, and pay off our debts by defining the general notion of resolution and proving the basic Lemma.

\section{Substitution, Unification and Resolution}

The following definitions are concerned with the operation of instantiation, i.e. substitution of terms for variables in well-formed expressions and in sets of well-formed expressions, and with the various auxiliary notions needed to define resolution in general.

\subsection{Substitution components.} A substitution component is any expression of
\newpage

\noindent the form $T/V$, where $V$ is any variable and $T$ is any term different from $V$. $V$ is
called the \emph{variable} of the component $T/V$, and $T$ is called the \emph{term} of the component $T/V$.

\subsection{Substitutions.}

A substitution is any finite set (possibly empty) of substitution components none of the variables of which are the same. If $P$ is any set;
of terms, and the terms of the components of the substitution $\theta$ are all in $P$, we say" that $\theta$ is a substitution over $P$. We write the substitution whose components are $T_1/V_1, \dotsm , T_k/V_k as \{T_1/V_1, \dotsm, T_k/V_k\}$, with the understanding that the order of the components is immaterial. We use lowercase Greek letters to denote substitutions. In particular, $\epsilon$ is the \emph{empty substitution}.

\subsection{Instantiation.}

IF E is any finite string of symbols and

\begin{align*}
    \theta = \{T_1/V_1, \dotsm, T_k/V_k\}
\end{align*}

%TODO double check the subscript
is any substitution, then the instantiation of $E$ by $\theta$ is the operation of replacing each occurrence of the variable $V_i$, $1 \leq i \leq k$, in $E$ by an occurrence of the term $T_i$. The resulting string, denoted by $E\theta$, is called the instance of $E$ by $\theta$. I.e., if $E$ is the string $E_0V_{i_1}E_1 \dotsm V_{i_n}E_n$ then $E\theta$ is the string $E_\theta T_{i_1}E_1\dotsm T_{i_n}E_n$. Here, none of the substrings $E_i$ of $E$ contain occurrences of the variables $V_1, \dotsm , V_k$ some of the $E_i$ are possibly null, $n$ is possibly $0$, and each $V_{i_i}$ is an occurrence of one of the variables $V_i,\dotsm V_k$. Any string $E\theta$ is called an instance of the
string $E$. If $C$ is any set of strings and $\theta$ a substitution, then the instance of $C$ by $\theta$ is the set of all strings $E0$, where $E$ is in $C$. We denote this set. by $C\theta$, and say that it is an instance of $C$.

\subsection{Standardization.}

If $C$ is any finite set of strings, and $V_1, \dotsm , V_k$ are
all the distinct variables, in alphabetical order, which occur in strings in $C$, then
the $x$-standardization of $C$, denoted by $\xi_c$, is the substitution
$x_1/V_1,\dotsm,x_k/V_k$
and the $y$-standardization of $C$, denoted by $\eta_c$, is the substitution
\begin{align*}
    \{y_1/V_1,\dotsm,y_k/V_k\}
\end{align*}

\subsection{Compositions of substitutions.}

If $\theta = \{T_1/V_1,\dotsm T_k/V_k\}$ and $\lambda$ are any
two substitutions, then the set $\theta^\prime \cup \lambda^\prime$ ,where $\lambda^\prime$ is the set of all components of $\lambda$
whose variables are not among $V_1,\dotsm V_k$, and $\theta^\prime$ is the set of all components
$T_i\lambda/V_i, 1 \leq i \leq k$, such taht $T_i\lambda$ is different from $V_i$, is called the composition of $\theta$ and $\lambda$, and is denoted by $\theta\lambda$

It is straight forward to verify that $\epsilon\theta = \theta\epsilon = \theta$ for any substitution $\theta$. Also,
composition of substitutions enjoys the associative property $(\theta\lambda)\mu = \theta(\lambda\mu)$, so
that we may omit parentheses in writing multiple compositions of substitutions. 
These properties of the composition of substitutions are established by the following propositions.

\subsubsection{}
$(E\sigma)\lambda = E(\sigma\lambda)$ \emph{for all strings E and all} substitutions $\sigma, \lambda$

\begin{proof}
Let $\sigma = \{T_1/V_1,\dotsm,T_k/V_k\}$, $\lambda = \{U_1/W_1,\dotsm,U_m/W_m\}$ and\\$E = E_oV_i,E_1,\dotsm V_{i_n}E_n$ as explained in (5.3) above. Then by definition $E\sigma =$
\end{proof}

%TODO use this as a ref for the hard to read subscript
\newpage
$E_0T_{1_1}E_1\dotsm T_{i_n}E_n$, and $(E\sigma)\lambda = \overline{E}_0\overline{T}_{i_1}\overline{E}_1\dotsm\overline{T}_{i_n}\overline{E}_n$, where each $\overline{T}_{i_i}$ is $T_{i_i}\lambda_i$ and each $\overline{E}_i$ is $E_i\lambda^\prime$, where $\lambda^\prime$ is the set of all components of $\lambda$ whose variables are not among $V_1,\dotsm,V_k$ (since none of these variables occur in any $E_i$). But $\sigma\lambda = \sigma^\prime \cup \lambda^\prime$, where each component of $\sigma\prime$ is just $\overline{T}_i/V_i$ whenever $\overline{T}_i$ is different from $V_i$. Hence $E(\sigma\lambda) = \overline{E}_0\overline{T}_{i_i}\overline{E}_1\dotsm\overline{T}_{i_n}\overline{E}_n$

\subsubsection{}
\emph{For any substitutions $\sigma$, $\lambda$: if $E\sigma = E\lambda$ for all strings $E$, then $\sigma = \lambda$}
\begin{proof}
Let $V_1,\dotsm,V_k$ include all the variables of the components of $\sigma$ and $\lambda$; then $V_j\sigma = V_j\lambda$, for $1 \leq j \leq k$. Then all the components of $\sigma$ and $\lambda$ are the same.
\end{proof}

\subsubsection{}
\emph{For any substitutions $\sigma, lambda, mu: (\sigma\lambda)\mu = \sigma(\lambda\mu)$}
\begin{proof}
Let $E$ be any string. Then by 5.5.1,
\begin{align*}
    E((\sigma\lambda)\mu) &= (E(\sigma\lambda))\\
                          &= ((E\sigma)\lambda)\mu\\
                          &= (E\sigma)(\lambda\mu)\\
                          &= E(\sigma(\lambda\mu)).
\end{align*}
\end{proof}

\noindent Hence $(\sigma\lambda)\mu = \sigma(\lambda\mu)$ by (5.5.2).

We shall also have occasion to use the following distributive property. 
\subsubsection{}
\emph{For any sets $A$, $B$ of strings and substitution $\lambda: (A \cup B)\lambda = A\lambda\cup B\lambda$}

\subsection{Disagreement sets.}

If $A$ is any set of well-formed expressions, we call the
set $B$ the disagreement set of $A$ whenever $B$ is the set of all well-formed subexpressions of the well-formed expressions in $A$, which begin at the first symbol position at which not all well-formed expressions in $A$ have the same symbol.
\noindent Example:

\begin{align*}
                           A &= \{P(x,h(x,y),y),P(x,k(y),y), P(x,a,b)\}\\
\text{Disagreement set of} A &= \{h(x,y), k(y), a\}.
\end{align*}

Evidently, if $A$ is nonempty and is not a singleton, then the disagreement set
of $A$ is nonempty and is not a singleton. 

\subsection{Unification.}
If $A$ is any set of well-formed expressions and $\theta$ is a substitution, then $\theta$ is said to unify $A$, or to be a unifier of $A$, if $A\theta$ is a singleton. Any set of well-formed expressions which has a unifier is said to be unifiable. 

Evidently, if $\theta$ unifies $A$, but $A$ is not a singleton, then $\theta$ unifies the disagreement set of $A$.

\subsection{Unification Algorithm.}
The following process, applicable to any finite
nonempty set $A$ of well-formed expressions, is called the Unification Algorithm:

%https://tex.stackexchange.com/a/341190
\begin{steps}
    \item Set $\sigma_0 = \epsilon$ and $k = 0$, and go to step 2.
    \item If $A\sigma_k$ is not a singleton, go to step 3. Otherwise, set $\sigma_A = \sigma_k$ and terminate.
    \item Let $V_k$ be the earliest, and $U_k$ the next earliest, in the lexical ordering of the disagreement set $B_k$ of $A\sigma_k$ If $V_k$ is a variable, and does not occur in $U_k$, set $\sigma_{k+1} = \sigma_k\{U_k/V_k\}$, add $1$ to $k$, and return to step 2. Otherwise, terminate.
\end{steps}

This definition requires justification in the form of a proof that the given
process is in fact an algorithm. In fact the process always terminates for any

\newpage

TODO fix all set braces

TODO REMOVE
\cite{church_1936}
\cite{davis_1960}
\cite{friedman_1963}
\cite{gilmore_1960}
\cite{robinson_1963}

\bibliographystyle{acm}
\bibliography{resolution.bib}
\end{document}
